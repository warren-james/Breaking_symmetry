---
title: "Supplementary Material for Asymmetry paper"
output: 
  bookdown::html_document2:
    theme: journal
    toc: true
bibliography: ["Literature.bib"]
biblio-style: "apalike"
link-citations: true
---

---
title: "Supplementary Material For Asymmetry paper"
output: html_document
---

```{r settings, include = F, echo = F}
knitr::opts_chunk$set(fig.align = "center")
```

```{r setup, include=T, message=F, warning=F, echo= F}
# library 
library(tidybayes)
library(tidyverse)
library(brms)
library(png)
library(grid)
```

These supplementary materials present full details of all R packages used in the analysis; a detailed explaination of our simulation-based power analysis; and Bayesian models of the results discussed in the main manuscript. In addition, we also present details a further, related experiment using eye-tracking and a probability matching paradigm. 

Please see the accompanying source Rmd file for the code. 

# Session Info

```{r, include=T, echo=T}
sessionInfo()
```

``` {r, echo = F, include = F}
options(mc.cores = parallel::detectCores())
```
# Power Analysis

Our power analysis made use of simulations and resampling approaches based on data that we previously collected. Resampling was carried out in order to establish if adding more participants to our sample sizes would have lead to a reduction in uncertainty around our estimates.

## Experiment 1: Two Hoop Sizes
We started by fitting a a beta distribution to each participant's date from the _Throwing Experiment_ in @clarke2015failure using the `fitdistrplus` package (as can be seen in Figure \@ref(fig:drawdistributions). In the original data, normalised standing position was coded $\phi \in [0,1]$ with $0$ indicating a central standing position, and $1$ indicating the participant stood by either of the two hoops. As we are _breaking the symmetry_ between the two targets in this new experiment, we will now treat standing position as $\phi \in (0, 1)$ with $0$ and $1$ representing the positions of the large and small hoop respectively. The central midpoint is now represented as $\phi = 0.5$.

```{r drawdistributions, include = T, echo = F, fig.height = 5, fig.cap="_These plots show the various distributions that were fit to each participant. These were then used to draw random samples from in order to simulate participants shifting towards one target._", fig.align = 'center'}
# load("../power/Hoop_size/scratch/sampled_data")
load("../power/Hoop_size/scratch/df_part2")

# functions for getting parameters from a beta dist
mu_beta <- function(a, b){
  a/(a + b)
}
phi_beta <- function(a, b){
  1/((a*b)/(((a+b)^2)*(a+b+1)))
}
var_beta <- function(a, b){
  (a*b)/(((a+b)^2)*(a+b+1))
}

# get shape parameters 
get_shape_beta <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

# sort out beta shape
shape_1 <- fitdistrplus::fitdist(df_part2$beta_pos, "beta")
mu1 <- mu_beta(shape_1$estimate[1], shape_1$estimate[2])
var1 <- var_beta(shape_1$estimate[1], shape_1$estimate[2])
shape_1 <- get_shape_beta(mu1, var1)
mu2 <- seq(0.05, .2, 0.05)
x <- seq(0.02,1-0.02,0.02)

# draw these 
df_distributions <- tibble(x_vals = rep(x, length(c(0,mu2))),
                           base = mu1,
                           diff = rep(c(0,mu2), each = length(x)),
                           var = var1,
                           alpha = get_shape_beta(base + diff, var)$alpha,
                           beta = get_shape_beta(base + diff, var)$beta,
                           est_mu = mu_beta(alpha, beta),
                           est_var = var_beta(alpha, beta),
                           p = dbeta(x_vals, alpha, beta))

plt_both <- df_distributions %>% 
  mutate(diff = as.factor(diff)) %>%
  ggplot(aes(x_vals, p)) + 
  geom_histogram(data = df_part2, 
                 aes(x = (norm_pos + 1)/2,
                     y = ..density..),
                 binwidth = .1,
                 colour = "blue",
                 fill = "blue",
                 alpha = .2) + 
  geom_line(aes(colour = diff)) + 
  theme_bw() + 
  see::scale_color_flat() +
  guides(colour = guide_legend("Difference")) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "bottom")

plt_dists <- df_distributions %>% 
  mutate(diff = as.factor(diff)) %>% 
  ggplot(aes(x_vals, p)) + 
  geom_line(aes(colour = diff)) + 
  theme_bw() + 
  see::scale_color_flat() +
  scale_x_continuous("standing position") + 
  guides(colour = guide_legend("Difference")) + 
  theme(
    axis.title.y = element_blank(),
    legend.title = element_blank(),
    legend.position = "right"
  )

plt_hist <- df_distributions %>% 
  filter(diff == 0) %>% 
  mutate(diff = as.factor(diff)) %>% 
  ggplot(aes(x_vals, p)) + 
  geom_line(aes(colour = diff)) + 
  see::scale_color_flat() + 
  theme_bw() + 
  geom_histogram(data = df_part2, 
                 aes(x = (norm_pos + 1)/2, y = ..density..),
                 binwidth = .1,
                 colour = "blue", 
                 fill = "blue",
                 alpha = .2) + 
  scale_x_continuous("standing position") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank()
  )

# gridExtra::grid.arrange(plt_hist, plt_dists, ncol = 2)


# Same as above but for each participant
# Need to sample each participant and get parameters for them 
temp <- df_part2 %>% 
  group_by(Participant) %>% 
  summarise(a = fitdistrplus::fitdist(beta_pos, "beta")$estimate[1],
            b = fitdistrplus::fitdist(beta_pos, "beta")$estimate[2])
mus <- seq(0, .2, .05)

df_distributions <- tibble(participant = rep(temp$Participant, each = length(mus)),
                           og_as = rep(temp$a, each = length(mus)),
                           og_bs = rep(temp$b, each = length(mus)),
                           og_mu = mu_beta(og_as, og_bs),
                           og_var = var_beta(og_as, og_bs),
                           increase = rep(mus, length(temp$a))) %>% 
  mutate(mu = og_mu + increase,
         var = og_var, 
         alpha = get_shape_beta(mu, var)$alpha,
         beta = get_shape_beta(mu, var)$beta) %>% 
  select(participant, increase, mu, var, alpha, beta)

# make some plots
df_dists <- tibble(Participant = rep(unique(df_distributions$participant), each = length(x) * length(mus)),
                   x_vals = rep(x, length(mus) *length(unique(df_distributions$participant))),
                   increase = rep(rep(mus, each = length(x)), length(unique(df_distributions$participant))),
                   alpha = rep(unique(df_distributions$alpha), each = length(x)),
                   beta = rep(unique(df_distributions$beta), each = length(x))) %>% 
  mutate(increase = as.factor(increase)) %>% 
  group_by(Participant, increase, x_vals) %>% 
  mutate(y = dbeta(x_vals, alpha, beta)) %>% 
  filter(increase %in% c(0, .05))


# make plot 
plt_dists_wP <- df_part2 %>%
  ggplot(aes(beta_pos)) + 
  geom_histogram(aes(y = ..density..),
                 binwidth = .1
  ) + 
  geom_line(data = df_dists,
            aes(x_vals, y,
                colour = increase)) + 
  see::scale_color_flat() + 
  scale_x_continuous(expression(paste("Normalised ", Delta, sep = " ")), breaks = c(0, 0.5, 1)) +
  theme_bw() + 
  theme(strip.text.x = element_blank()) + 
  facet_wrap(~Participant, scales = "free")
plt_dists_wP
```

The fit with the empirical data is reasonable, although we under-estimate the frequency of standing positions $\phi\approx0.5$, in this case 0.5 is considered the central point with 0 and 1 representing the left and right hoop respectively. We assumed that by introducing a difference in size between the two hoops that this distribution would shift towards the smaller hoop so as to balance their chances of success. We assumed that the difference in hoop size would cause particiapnts to stand slightly closer to the small hoop, in this case the shift in mean would be +0.05. 
```{r Hoop Size resampling, include = T, echo = F, cache = T}

# setup loop
diff_sample <- seq(.05, .2, .05)
n_iter <- 5000
n_subs <- seq(3, 25, 1)
n_trials <- 72
n <- length(diff_sample) * n_iter * length(n_subs)
refresh <- n_iter/100

df_sample <- data.table::data.table(iter = rep(0, n),
                                    increase = rep(0, n),
                                    n_subs = rep(0, n),
                                    n_trial = rep(0, n),
                                    base = rep(0, n),
                                    comp = rep(0, n))

# loop 
count <- 1 

for(ii in 1:n_iter){
  for(inc in diff_sample){
    for(subN in n_subs){
      subs <- sample(unique(df_distributions$participant), subN, replace = T)
      baseline <- c()
      comparison <- c()
      for(sub_iter in 1:length(subs)){
        base_a <- df_distributions$alpha[df_distributions$participant == subs[sub_iter] &
                                           df_distributions$increase == 0]
        base_b <- df_distributions$beta[df_distributions$participant == subs[sub_iter] &
                                          df_distributions$increase == 0]
        comp_a <- df_distributions$alpha[df_distributions$participant == subs[sub_iter] &
                                           df_distributions$increase == inc]
        comp_b <- df_distributions$beta[df_distributions$participant == subs[sub_iter] &
                                          df_distributions$increase == inc]
        baseline <- c(baseline, rbeta(n_trials, base_a, base_b))
        comparison <- c(comparison, rbeta(n_trials, comp_a, comp_b))
      }
      df_sample[count, iter := ii]
      df_sample[count, increase := inc]
      df_sample[count, n_subs := subN]
      df_sample[count, n_trial := n_trials]
      df_sample[count, base := mean(baseline)]
      df_sample[count, comp := mean(comparison)]
      count <- count + 1
    }
  }
  # if(ii %% refresh == 0){
  #   print(paste((ii/n_iter)*100, "%", sep = ""))
  # }
}

```

We can now these use distributions to simulate experiments with $N = 3\ldots 24$ participants and 72 trials. Figure \@ref(fig:HDIHoopSize) shows the uncertainty surrounding the mean estimate for the smallest difference tested (5\%). After 15 participants, the uncertainty surrounding the estimate appears to plateau which demonstrates that the sample size of 21 was sufficient to detect an effect of this size. 

```{r HDIHoopSize, include = T, echo = F, fig.align = 'center', fig.height = 3, fig.cap="_This figure shows how the 95% HDI around the mean difference for the smallest difference simulated (5%) changed with a larger sample size_"}

df_sample %>% 
  filter(increase == 0.05) %>% 
  group_by(n_subs) %>% 
  mutate(diff = comp - base) %>% 
  summarise(mu = mean(diff), 
            lower = hdi(diff)[1],
            upper = hdi(diff)[2]) %>% 
  ggplot(aes(n_subs, mu)) + 
  geom_line() + 
  geom_ribbon(aes(ymin = lower,
                  ymax = upper),
              alpha = .3) + 
  scale_x_continuous("Sample Size") + 
  theme_bw() + 
  theme(axis.title.y = element_blank())

```

## Experiment 2: Two Throws

Our hypothesis for this experiment was that our intervention would push participants towards being more optimal. For the power analysis, we will sample data from two unpublished versions of the split-resource decision problem. For the _One Throw_ (control) condition, we will use data from a (as yet unpublished) replication of the @clarke2015failure _Throwing Task_ for $N=40$ participants. As with the original study, participants were generally sub-optimal and did not vary their strategy with $\Delta$. 

The second dataset is comprised of 60 participants who took part in a computerised version of this task. The task involved participants deciding where to place a virtual firetruck when there were two buildings which both had an equal chance of becoming the target for that trial. In this task, participants would decide where to place the firetruck and it would drive towards the target building. The speed at which the truck travelled was drawn from a uniform distribution of speeds within a specified range. This meant that the chance of success decrease linearly with distance from the target. This meant that the performance of the truck was very similar to participatns taking part in the _Throwing Task_, and thus the experiments were comparable. However, participants in this version of the task were much closer to using the optimal strategy than had been observed in previous versions of this task. Therefore, these data were used as a comparison for what to expect when participants were better able to use the optimal strategy. 

To compare these datasets, placement positions and standing positions were put on the same scale (0 being the centre and 1 being stood/placed next to the target). Only data for the easiest (smallest separation of targets) and hard (furthest separation of targets) conditions were considered as these points offered more comparable base performance levels. See Figure \@ref(fig:firetruck-data) for details.

```{r firetruck-data, include = T, fig.height = 3, echo = F, fig.align = 'center', fig.cap="_boxplots for the empirical data being sampled from._"}
# so we're interested in the difference between the closest and furthest... 
# so we should just plot that? 

# load data
load("../power/Money_and_two/scratch/df_all")

# make plot 
plt_data <- df_all %>%
  filter(standard_lab %in% c("10%","90%")) %>%
  group_by(participant, condition, standard_lab) %>% 
  summarise(norm_pos = mean(norm_pos)) %>%
  ggplot(aes(standard_lab, norm_pos,
             colour = condition,
             fill = condition)) + 
  geom_boxplot(alpha = .3) + 
  geom_point(alpha = .3, position = position_jitterdodge(.1)) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  theme_bw() +
  scale_y_continuous("Normalised Placement") +
  theme(axis.title.x = element_blank()) + 
  guides(colour = guide_legend(title = "Condition"),
         fill = guide_legend(title = "Condition"))

plt_data_diff <- plt_data[["data"]] %>%
  ungroup() %>% 
  spread(standard_lab, norm_pos) %>% 
  mutate(diff = `90%` - `10%`) %>% 
  ggplot(aes(condition, diff,
             colour = condition, 
             fill = condition)) + 
  geom_boxplot(alpha = .3) + 
  geom_point(alpha = .3, position = position_jitterdodge(.1))

plt_data
```

To perform our power analysis we sampled these datasets 5000 times for a range of participants ($N = 2...28$). To do this, N participants were chosen at random (with replacement) from each condition (i.e. the _Throwing_ or _Avatar_ "conditions"), then 60 data points were sampled from each of these participants (with replacement) for the closest ("90%") and furthest separations ("10%").  

``` {r two-throw power, include = F, echo = F, cache = T}
# setup loop parameters
n_iter <- 5000
n_subs <- seq(2, 28, 2) 
n_trials <- c(12, 60)
dists <- c("90%", "10%")

# setup data 
n <- n_iter * length(n_subs) * length(n_trials) * length(dists)

df_sample <- data.table::data.table(iter = rep(0, n),
                                    n_subs = rep(0, n),
                                    n_trials = rep(0, n),
                                    dist_type = rep("", n),
                                    baseline = rep(0, n),
                                    comparison = rep(0, n))
# 
# # TODO make this loop quicker by using data.table()
# # split dataset for ease of access...
df_Av <- df_all %>%
  filter(condition == "Avatar")
df_Th <- df_all %>%
  filter(condition == "Throwing")

# start counter
count <- 0
# loop through
for(ii in 1:n_iter){
  # loop distances
  for(dist in dists){
    base_ss <- df_Th %>%
      filter(standard_lab == dist)
    comp_ss <- df_Av %>%
      filter(standard_lab == dist)
    for(trials in n_trials){
      for(subs in n_subs){
        # get random subjects
        base_subs <- sample(df_Th$participant, subs, replace = T)
        comp_subs <- sample(df_Av$participant, subs, replace = T)
        
        # loop through subjects
        base_sample <- c()
        comp_sample <- c()
        for(sub in 1:subs){
          base_temp <- base_ss %>%
            filter(participant == base_subs[sub])
          comp_temp <- comp_ss %>%
            filter(participant == comp_subs[sub])
          base_sample <- c(base_sample,
                           sample(base_temp$norm_pos, trials, replace = T))
          comp_sample <- c(comp_sample,
                           sample(comp_temp$norm_pos, trials, replace = T))
          if(sub == subs){
            count <- count + 1
            # input into table
            df_sample[count, iter := ii]
            df_sample[count, n_subs := subs]
            df_sample[count, n_trials := trials]
            df_sample[count, dist_type := dist]
            df_sample[count, baseline := mean(base_sample)]
            df_sample[count, comparison := mean(comp_sample)]
          }
        }
      }
    }
  }
}

```

```{r exp2-power, include = T, fig.height = 3, echo = F, fig.align = 'center', fig.cap="_plots show the 95% HDI for the difference in the close (90%) and far (10%) separations_"}
# plot samples in terms of difference? 
plt_hdi <- df_sample %>% 
  filter(n_trials != 12,
         n_subs != 2) %>%
  mutate(diff = baseline - comparison) %>% 
  group_by(n_subs, dist_type) %>% 
  summarise(mu = mean(diff),
            upper = hdi(diff)[2],
            lower = hdi(diff)[1]) %>% 
  ggplot(aes(n_subs, mu)) + 
  geom_line() + 
  geom_ribbon(aes(ymin = lower, ymax = upper),
              alpha = .3) + 
  facet_wrap(~dist_type) + 
  theme_bw() + 
  scale_x_continuous("Sample Size") + 
  scale_y_continuous("Difference: Athletes - Avatar")

plt_hdi
```

## Experiment 3: Reward 
The power analysis for this experiment followed that of the previous section. This was due to the similarity of the hypothesis; the goal was to test whether introducing an asymmetry in the reward structure would encourage participants to adopt a more optimal strategy.

## Experiment 4: Probability Matching
```{r load data, include = F, echo = F, message = F, warning = F}
load("../power/Prob_match/scratch/df_resample")
```

For this experiment, data from ($N$ = 12) @clarke2015failure, which acted as a substitute for the Symmetric condition, and an unpublished pilot study ($N$ = 11), which followed the same rules as the Bias condition in the main experiment, were resampled in order to invesigate how often participants would fixate one of the side boxes given it was more likely to contain the target for the Bias condition. For the Symmetric condition, we followed the same rule as the in the paper and classified the most likely box as the side box that participants fixated most often (Figure \@ref(fig:ProbOGData). The main interest for our purposes was the proportion of time participants fixated the most likely box.

```{r ProbOGData, include=T, echo=F, fig.asp=.45, fig.cap="_Boxplots to show the proportion of trials in which the participants fixated the each box by Condition and Distance type_"}
plt_data <- df_resample %>%  
  ungroup() %>% 
  group_by(Participant, Condition, Dist_type) %>% 
  mutate(centre = ifelse(box_type == "Centre", 1, 0),
         ML = ifelse(box_type == "Most", 1, 0),
         LL = ifelse(box_type == "Least", 1, 0),
         n = n()) %>%
  summarise(centre = mean(centre),
            ML = mean(ML),
            LL = mean(LL)) %>% 
  gather(centre:LL,
         key = "prop_type",
         value = "proportion") %>%
  mutate(prop_type = factor(prop_type, c("centre", "ML", "LL"),
                            labels = c("Centre", "Most Likely", "Least Likely"))) %>% 
  ggplot(aes(Dist_type, proportion,
             fill = Condition,
             colour = Condition)) + 
  geom_boxplot(alpha = .3) + 
  geom_point(alpha = .3, position = position_jitterdodge(.1)) +
  facet_wrap(~prop_type) +
  see::scale_color_flat() +
  see::scale_fill_flat() +
  theme_bw() +
  scale_x_discrete("Distance Type") +
  theme(axis.title.y = element_blank())
plt_data
```

The data were coded so that a fixation was classified as either being to the Most likely box or not. We then resampled (with replacement) these data by selecting a random $N$ participants from each condition (ranging from 2 to 20) then sampling 300 trials from these participants from the different datasets. This was done 5000 times to estimate the expected difference between the Symmetric and Bias conditions in terms of proportion of fixations to the Most likely side and the associated certainty around these values.

As can be seen in the figure below, the uncertainty surrounding the estimate for the difference between the groups appears to plateau somewhere around 15 participants. The shaded region represents a 95% Highest Density Interval (HDI) for the disribution of differences simulated through resampling. As such, the sample size of 18 in the main experiment appears to be ample in order to detect this difference and increasing the sample size above this value does not add more to the certainty, as can be seen in Figure \@ref(fig:ProbPowerPlots). 

``` {r prob matching power, include = F, echo = F, cache = T}
load("../power/Prob_match/scratch/df_resample")

# sampling
n_trials <- 300 # seq(100, 450, 50)
n_subs <- seq(2, 20, 1)
n_iter <- 5000
conditions <- unique(df_resample$Condition)

# setup data.table 
n <- length(n_subs) * length(conditions) * n_iter
df_sample <- data.table::data.table(iter = rep(0, n),
                                    n_sub = rep(0, n),
                                    n_trials = rep(0, n),
                                    Condition = rep("", n),
                                    Most = rep(0, n))

# run loop
counter <- 0
for(ii in 1:n_iter){
  # loop conditions
  for(cond in conditions){
    ss_cond <- df_resample %>%
      filter(Condition == cond)
    
    # loop n trials
    for(trials in n_trials){
      # loop n subjects
      for(sub in n_subs){
        # get random participants
        subj <- sample(ss_cond$Participant, sub, replace = T)
        
        # get data
        ss_sub <- ss_cond %>%
          filter(Participant %in% subj)
        
        # set this for a stopping point
        # final_sub <- tail(unique(ss_sub$Participant), n = 1)
        
        # initialise vector
        samp <- c()
        
        # loop through participants
        count <- 0
        for(unique_sub in subj){
          count <- count + 1
          # who are we sampling from?
          sampling <- ss_sub %>%
            filter(Participant == unique_sub)
          
          # sample random trials
          samp <- c(samp, sample(sampling$Most, trials, replace = T))
          
          # check if we're adding this to the data frame
          if(count == length(subj)){
            counter <- counter + 1
            df_sample[counter, iter := ii]
            df_sample[counter, n_sub := sub]
            df_sample[counter, n_trials := trials]
            df_sample[counter, Condition := cond]
            df_sample[counter, Most := mean(samp)]
          }
        }
      }
    }
  }
}



```

```{r ProbPowerPlots, include = T, echo = F, fig.asp=.45, fig.cap="_Plot to show how the 95% HDI of the estimated difference in fixations to the most likely box changed with an increase in participants_"}
# maybe just the hdi stuff?
# do I want to rerun the sim with dist type added in?
plt_resampling <- df_sample %>% 
  spread(Condition, 
         Most) %>% 
  mutate(diff = Bias - Symmetric) %>% 
  group_by(n_sub) %>% 
  summarise(mu = mean(diff),
            lower = hdi(diff)[1],
            upper = hdi(diff)[2]) %>% 
  ggplot(aes(n_sub, mu)) + 
  geom_line() + 
  geom_ribbon(aes(ymin = lower,
                  ymax = upper),
              alpha = .3) +
  theme_bw() + 
  scale_x_continuous("Sample Size") + 
  scale_y_continuous("Differenc in fixations to more \n likely side (Bias - Symmetric)")

plt_resampling
```

# Additional Analyses

In the analyses below, we use multi-level Bayesian beta regression to model the change in standing position $\phi$ when we 1) use unequal hoop sizes, 2) remove the need to chose between hoops, and 3) provide unequal rewards. 

## Experiment 1: Hoop Size

```{r load data and source function Hoop Size, echo - FALSE}
# load model data 
load("../Analyses/Experiment_3_Hoop_size/data/model_data")

# get function for extracting draws 
source("extract_draws_functions/Hoop_size_draws.R")

```

Add in facet plots 

```{r HoopSizeFacets, include=T, echo=F}
load("../Analyses/Experiment_3_Hoop_size/scratch/df_part2_norm")
df_hoopsize <- norm_dat
rm(norm_dat)

# process data 
df_hoopsize %>% 
  group_by(participant) %>% 
  mutate(temp = as.numeric(as.factor(hoop_pos)),
         slab_measures = factor(temp, labels = c("~90%", "~50% - 1", "~50%", "~50% + 1", "~50% + 2", "~10%"))) %>% 
  select(-temp) %>%
  ungroup() %>% 
  filter(abs(norm_dist) <= 1) %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(hoop_pos))-> df_hoopsize

# make plot
df_hoopsize %>% 
  ggplot(aes(overall_hoop_pos, norm_dist)) + 
  geom_jitter(alpha = .2,
              width = .2,
              height = .2) +
  facet_wrap(~participant, ncol = 3) + 
  geom_hline(yintercept = 0, 
             linetype = "dashed") +
  scale_y_continuous("", 
                     breaks = c(-1, 0, 1),
                     labels = c("Big Hoop", "Centre", "Small Hoop")) +
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")")),
                     expand = c(0.1,0.1),
                     breaks = unique(df_hoopsize$overall_hoop_pos),
                     labels = c("-1", "0", "+1", "+2", "F", "C")) +
  theme_bw() + 
  theme(strip.text.x = element_blank(),
        panel.grid.minor = element_blank())

```



The data from this experiment were analysed using a Bayesian beta regression. The recorded data for standing positions were transformed to be between 0 and 1, with 0 representing the larger hoop and 1 representing the smaller hoop. Therefore, the central point would be 0.5, and meaning anything above this value would demonstrate a shift in participant’s bhaviour away from the mid-point and towards the small hoop. The model included normalised hoop delta as a predictor to see how participants changed position with an increasing distance. This was also entered as a random effect by participant. 


### Prior Predictions

First, we will check that our priors are sensible! (i.e., reasonably flat?)

```{r run-model-hoop-size-priors, echo = T, include = F, cache = TRUE}

# run model
Hoop_size_priors <- brm(norm_dist2 ~ norm_hoop_pos + (norm_hoop_pos|participant), 
                        data = model_data,
                        family = "beta",
                        prior = c(set_prior("student_t(3, 0, 1)",
                                            coef = "norm_hoop_pos",
                                            class = "b"),
                                  set_prior("student_t(3,0,1)",
                                            class = "Intercept")),
                        cores = 2,
                        chains = 2,
                        iter = 5000,
                        warmup = 2500,
                        control = list(adapt_delta = .95, max_treedepth = 15),
                        sample_prior = "only")
```

```{r HoopSizePriors, include = T, echo = F, warning = F,  fig.asp = .45, fig.cap="_Prior predictions for the Hoop Size experiment_"}
# need to make a dataset
draws <- HS_draw_post(Hoop_size_priors, model_data)

plt_estimates <- draws$plt_estimates 
plt_estimates

```

### Conditioning on the data

```{r run-model-Hoop-size, include = F, echo = T, cache = TRUE}

# run model
Hoop_size_m1 <- brm(norm_dist2 ~ norm_hoop_pos + (norm_hoop_pos|participant), 
                    data = model_data,
                    family = "beta",
                    prior = c(set_prior("student_t(3, 0, 1)",
                                        coef = "norm_hoop_pos",
                                        class = "b"),
                              set_prior("student_t(3,0,1)",
                                        class = "Intercept")),
                    cores = 2,
                    chains = 2,
                    iter = 5000,
                    warmup = 2500,
                    control = list(adapt_delta = .95, max_treedepth = 15))

```

```{r Extract Draws, include = F, echo = F, cache=TRUE}
# get draws
draws <- HS_draw_post(Hoop_size_m1, model_data)
```




```{r Sort Hdis, include = F, echo = F}
p_above_5 <- draws$Prop_above.5
HDI_all <- draws$draws_HDI[[2]]
```

The model results confirmed that participants in general had a bias towards standing closer to the small hoop (mean of `r round(HDI_all$mean, digits = 3)`, 95% HDPI of |`r round(HDI_all$lower, digits = 3)`, `r round(HDI_all$upper, digits = 3)`|). We can be reasonably confident about this results as the p(x > 0.5|data) = `r paste(round(p_above_5$above0_5, digits = 3)*100, "%", sep = "")`. This can be seen in the posterior in Figure \@ref(fig:PltHoopSizeEstimates). Also, not that distance did not appear to have an effect on position (i.e., participants were generally biased slightly towards the smaller hoop).

```{r PltHoopSizeEstimates, include = T, echo = F, fig.asp=.45, fig.cap="_Posterior predictions for the mean standing position._"}
plt_estimates <- draws$plt_estimates
plt_estimates
```

## Experiment 2: Two Throws

```{r Two Throws: load data, include = F, echo = F}
load("../Analyses/Experiment_2_Two_throw/scratch/model_data_pos")

source("extract_draws_functions/Two_throw_draws.R")
```

Add in facet plot 

``` {r TwoThrowFacets, include = T, echo = F}
# process 
df_TwoThrow <- model_data_pos %>% 
  group_by(Participant) %>%
  mutate(temp = as.numeric(as.factor(HoopDelta)),
         slab_measures = factor(temp, labels = c("~90%", "~50% - 1", "~50%", "~50% + 1", "~50% + 2", "~10%"))) %>% 
  select(-temp) %>% 
  ungroup() %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(HoopDelta))

# make plot 
df_TwoThrow %>% 
  ggplot(aes(overall_hoop_pos, abspos,
             colour = Num_throws)) + 
  geom_jitter(alpha = .2,
              width = .2) + 
  facet_wrap(~Participant, ncol = 6) + 
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")")),
                     breaks = unique(df_TwoThrow$overall_hoop_pos),
                     labels = c("C", "+1","-1", "+2", "0", "F")) +
  scale_y_continuous("",
                     breaks = c(0,1),
                     labels = c("Centre", "Side")) +
  see::scale_color_flat() +
  theme_bw() + 
  guides(fill = guide_legend("No. Throws")) +
  theme(strip.text.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(size = 8),
        legend.position = "bottom")


```

A Bayesian Beta regression was carried out to investigate whether participants performed the task in a more optimal way when they were given the chance to attempt to throw at both targets. The predicted value was the Normalised standing position with 0 being central and 1 being next to one of the hoops. The predictors were the normalised distance of the hoops from the centre (norm_delta), the number of throws participants had (Num_throws), and the interactions between these two terms.

We first carry out a prior predictive check to confirm that our choice of priors are reasonable. 

```{r Two_Throws_Priors_only_model, include = F, echo = T, cache = T}

my_prior <- c(
  set_prior("student_t(4,0,.8)", class = "b", coef = "norm_delta"),
  set_prior("student_t(4,0,.8)", class = "b", coef = "norm_delta:Num_throwsTwoMThrows"),
  set_prior("student_t(4,0,.8)", class = "b", coef = "Num_throwsTwoMThrows"),
  set_prior("student_t(4,0,.8)", class = "Intercept"))

Two_throw_priors <- brm(
  abspos ~ norm_delta * Num_throws + (norm_delta * Num_throws|Participant),
  family = "beta",
  data = model_data_pos,
  prior = my_prior,
  cores = 1,
  chains = 1,
  iter = 10000,
  warmup = 5000,
  control = list(adapt_delta = .99, max_treedepth = 15),
  sample_prior = "only")
```

```{r PltTwoThrowPriors, include = T, echo = F, fig.asp=.45, fig.cap="_Prior Predictions for the Two Throw Experiment_"}
# get draws 
draws_df <- draw_post_delta(Two_throw_priors, model_data_pos)

priors_plt <- draws_df$Draws_df %>%
  mutate(prop = boot::inv.logit(estimate),
         Dist_type = factor(Dist_type, c("Close", "Mid", "Far"))) %>% 
  ggplot(aes(prop,colour = Num_throws, fill = Num_throws)) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  scale_x_continuous(expression(paste("Normalised ", Delta))) +
  geom_density(kernel = "biweight", alpha = .3) + 
  facet_wrap(~Dist_type) + 
  theme_bw() + 
  guides(fill = guide_legend(title = "No. Throws"),
         colour = guide_legend(title = "No. Throws"))

priors_plt
```

Next, we train the model on the data and plot the posterior distributions. 

```{r Two-throws modelling, include = F, echo = T, cache = T}

Two_throw_m1 <- brm(
  abspos ~ norm_delta * Num_throws + (norm_delta * Num_throws|Participant),
  family = "beta",
  data = model_data_pos,
  prior = my_prior, 
  cores = 1,
  chains = 1,
  iter = 10000,
  warmup = 5000,
  control = list(adapt_delta = .99, max_treedepth = 15))

# load("../Analyses/Experiment_2_Two_throw/modelling/model_outputs/m1_pos")
draws <- draw_post_delta(Two_throw_m1, model_data_pos)

```

```{r}

summary(Two_throw_m1)

```

Does $\hat{R} = 1$? Do we have a good number of effective samples?

If so, we can have a look at the model fit...

```{r PltTwoThrowEstimates, include = T, echo = F, fig.asp=.55, fig.cap="_The top plots show the Posterior distributions for the estimated mean standing position. The bottom plot shows the distribution of estimated difference between the two conditions for three difference distances_"}

# example distributions 
plt_examples <- draws$Plots[1][[1]][["data"]] %>% 
  mutate(Dist_Type = factor(Dist_type, c("Close", "Mid", "Far")),
         Num_Throws = ifelse(Num_throws == "One_throw", "One", "Two")) %>% 
  ggplot(aes(prop, 
             colour = Num_Throws,
             fill = Num_Throws)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  facet_wrap(~Dist_Type) + 
  theme_bw() + 
  scale_x_continuous(expression(Delta)) + 
  coord_cartesian(expand = F) + 
  guides(fill = guide_legend(title = "No. Throws"),
         colour = guide_legend(title = "No. Throws"))

# plt_difference 
plt_difference <- draws$Plots[3][[1]] + 
  theme_bw() + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous("Difference: One throw - Two throw") +
  guides(fill = guide_legend(title = "Distance"),
         colour = guide_legend(title = "Distance"))

# show this 
gridExtra::grid.arrange(plt_examples, plt_difference)
```

```{r Sort HDI, include = F, echo = F}
# get overall 
HDI_two_throw_overall_diff <- draws$HDIs[[2]]
HDI_two_throw_mean_pos_overall <- draws$HDIs[[1]]
mu <- HDI_two_throw_mean_pos_overall$mean
lower <- HDI_two_throw_mean_pos_overall$lower
upper <- HDI_two_throw_mean_pos_overall$upper

# get the closest 
HDI_tt_close <- draws$HDIs[[3]] %>% 
  filter(Dist_type == "Close")
mu_close <- HDI_tt_close$mean
lower_close <- HDI_tt_close$lower
upper_close <- HDI_tt_close$upper

HDI_tt_diff <- draws$HDIs[[4]]

P_close <- draws$Above_0[[2]] %>% 
  filter(Dist_type == "Close")
```



The analysis suggested that there was an overall greater tendency for participants in the One-throw condition to stand further from the centre (mean of `r round(mu[1], digits = 3)`, 95% HPDI of |`r round(lower[1], digits = 3)` , `r round(upper[1], digits = 3)`|) than when they were in the Two-throw condition (mean of `r round(mu[2], digits = 3)`, 95% HPDI of |`r round(lower[2], digits = 3)` , `r round(upper[2], digits = 3)`|) with P(One-throw > Two-throw|data) = `r paste(round(draws$Above_0[[1]], digits = 3)*100, "%", sep = "")`. 

This effect was the strongest for the closest separation which reflects the larger amount of variation in standing position with distance in the Two-throw condition (Figure \@ref(fig:PltTwoThrowEstimates). In general, when in the One-throw condition, participants stood further from the centre (mean of `r round(mu_close[1], digits = 3)`, 95% HPDI of |`r round(lower_close[1], digits = 3)` , `r round(upper_close[1], digits = 3)`|) than in the the Two-throw condition (mean of `r round(mu_close[1], digits = 3)`, 95% HPDI of |`r round(lower_close[1], digits = 3)` , `r round(upper_close[1], digits = 3)`|) with P(One-throw > Two_throw|data) = `r paste(P_close$above0*100, "%", sep = "")`. 


## Experiment 3: Reward
``` {r RewardLoadData, include = F, echo = F, warning = F}
load("../Analyses/Experiment_5_Unequal_Reward/scratch/data/model_data")

source("extract_draws_functions/Reward_draws.R")
```

Add in facets 

``` {r RewardFacets, include = T, echo = F}
df_Rewards <- model_data %>%
  group_by(Participant) %>%
  mutate(temp = as.numeric(as.factor(Norm_Delta)),
         slab_measures = factor(temp, labels = c("90%", "75%", "25%", "10%"))) %>% 
  select(-temp) %>% 
  ungroup() %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(Norm_Delta))

df_Rewards %>% 
  ggplot(aes(overall_hoop_pos, Norm_Dist,
             colour = Gamble_Type)) + 
  geom_jitter(alpha = .3,
              width = .05,
              height = .05) +
  facet_wrap(~Participant, ncol = 4) + 
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")")),
                     breaks = unique(df_Rewards$overall_hoop_pos),
                     labels = unique(df_Rewards$slab_measures)) +
  scale_y_continuous("",
                     breaks = c(0,1),
                     labels = c("Centre", "Side")) +
  see::scale_color_flat() +
  theme_bw() + 
  theme(
    strip.text.x = element_blank(),
    legend.position = "bottom",
    axis.text.x = element_text(size = 8),
    panel.grid.minor.x = element_blank()
  )


```

A Bayesian Beta Regression was carried out in order to investigate whether opting for an unequal reward structure in this task would facilitate the use of a more optimal strategy in the _Throwing Task_. The predictors in this model were whether the participant had opted for an _Equal_ or _Unequal_ (Gamble_Type), whether the hoops were _Close_ or _Far_ (dist_type), and the interaction between these. Additionally, random effects of participant were included for all predictors. The predicted value was the normalised standing position, with 0 representing the centre and 1 being next to one of the hoops.  
```{r GamblingPriorsOnly, include = F, echo = T, warning = F, cache = T}
Reward_priors <- brm(Norm_Dist ~ dist_type*Gamble_Type + (dist_type * Gamble_Type|Participant),
                     data = model_data,
                     family = "beta",
                     prior = c(set_prior("student_t(3,0,.5)",
                                         class = "b",
                                         coef = "dist_typefar"),
                               set_prior("student_t(3,0,.5)",
                                         class = "b",
                                         coef = "dist_typefar:Gamble_TypeUnequal"),
                               set_prior("student_t(3,0,.5)",
                                         class = "b",
                                         coef = "Gamble_TypeUnequal"),
                               set_prior("student_t(3,0,1)",
                                         class = "Intercept")),
                     chains = 1,
                     iter = 10000,
                     warmup = 5000,
                     sample_prior = "only",
                     control = list(adapt_delta = .99, max_treedepth = 15))
```


``` {r RewardPriorsPlt, include = T, echo = F, fig.asp = .45, fig.cap = "_Prior Predictions for the Reward Study_"}
draws_factor(Reward_priors) %>% 
  mutate(prop = boot::inv.logit(estimate)) %>% 
  ggplot(aes(prop,
             colour = Gamble_Type,
             fill = Gamble_Type)) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  geom_density(alpha = .3,
               kernel = "biweight") +
  scale_x_continuous(expression(paste("Normalised ", Delta))) +
  guides(colour = guide_legend(title = "Gamble Type"),
         fill = guide_legend(title = "Gamble Type")) +
  facet_wrap(~Dist_Type) + 
  theme_bw()
```

As the prior predictive checks demonstrated that the model was able to retrieve the prior, and the priors were "sensible", the model was conditioned on the raw data from the experiment. 

```{r RewardModel, include = F, echo = T, warning = F, cache = T}
# run model 
Reward_m1 <- brm(Norm_Dist ~ dist_type*Gamble_Type + (dist_type*Gamble_Type|Participant),
                 data = model_data,
                 family = "beta",
                 prior = c(set_prior("student_t(3,0,.5)",
                                     class = "b",
                                     coef = "dist_typefar"),
                           set_prior("student_t(3,0,.5)",
                                     class = "b",
                                     coef = "dist_typefar:Gamble_TypeUnequal"),
                           set_prior("student_t(3,0,.5)",
                                     class = "b",
                                     coef = "Gamble_TypeUnequal"),
                           set_prior("student_t(3,0,1)",
                                     class = "Intercept")),
                 chains = 1,
                 iter = 10000,
                 warmup = 5000,
                 control = list(adapt_delta = .99, max_treedepth = 15))

```

```{r PltRewardestimates, echo = F, inlude = T, warning = F, fig.asp=.75, fig.cap="_Posterior distributions for the Reward Study_"}
# get draws 
draws_df <- draws_factor(Reward_m1) %>% 
  mutate(prop = boot::inv.logit(estimate))

# make separate plots for main effects and interactions 
plt_dist <- draws_df %>% 
  ggplot(aes(prop, 
             colour = Dist_Type,
             fill = Dist_Type)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  theme_bw() +
  guides(colour = guide_legend(title = "Distance Type"),
         fill = guide_legend(title = "Distance Type")) +
  scale_x_continuous(expression(paste("Normalised ", Delta)))

plt_gamble <- draws_df %>% 
  ggplot(aes(prop, 
             colour = Gamble_Type,
             fill = Gamble_Type)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  theme_bw() +
  guides(colour = guide_legend(title = "Gamble Type"),
         fill = guide_legend(title = "Gamble Type")) +
  scale_x_continuous(expression(paste("Normalised ", Delta)))

plt_inter <- draws_df %>%
  ggplot(aes(prop, 
             colour = Gamble_Type,
             fill = Gamble_Type)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  theme_bw() +
  guides(colour = guide_legend(title = "Gamble Type"),
         fill = guide_legend(title = "Gamble Type")) +
  scale_x_continuous("Normalised Position") + 
  facet_wrap(~Dist_Type)

gridExtra::grid.arrange(plt_dist, plt_gamble, plt_inter)

```

```{r RewardHDI, include = F, echo = F}
Rewards_hdi_Dist <- draws_df %>% 
  group_by(Dist_Type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2])

Rewards_hdi_Gamble <- draws_df %>% 
  group_by(Gamble_Type) %>% 
  summarise(lower = hdci(prop)[,1],
            mu = mean(prop),
            upper = hdci(prop)[,2])

Rewards_hdi_interaction <- draws_df %>% 
  group_by(Gamble_Type, Dist_Type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2])

```

```{r Probxgreaterthany, echo = F, include = F}
Pabove0_Dist <- draws_df %>% 
  group_by(.iteration, Dist_Type) %>% 
  summarise(mu = mean(prop)) %>%
  spread(Dist_Type, mu) %>% 
  mutate(diff = Far - Close,
         above0 = ifelse(diff > 0, 1, 0))

Pabove0_int <- draws_df %>% 
  group_by(.iteration, Dist_Type, Gamble_Type) %>% 
  summarise(mu = mean(prop)) %>% 
  mutate(inter = paste(Dist_Type, Gamble_Type, sep = "_")) %>% 
  ungroup() %>% 
  select(mu, inter, .iteration) %>%
  spread(inter, mu) %>% 
  mutate(diff_Far = Far_Unequal - Far_Equal,
         diff_Close = Close_Unequal - Close_Equal,
         above0Far = ifelse(diff_Far > 0, 1, 0),
         above0Close = ifelse(diff_Close > 0, 1, 0))

```
As can be seen in Figure \@ref(fig:PltRewardestimates), participants in this task were more likely to stand towards one of the side hoops when the hoops were far apart (mean of `r round(Rewards_hdi_Dist[2,3], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_Dist[2,2], digits = 3)` , `r round(Rewards_hdi_Dist[2,4], digits = 3)`|) than when they were close together (mean of `r round(Rewards_hdi_Dist[1,3], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_Dist[1,2], digits = 3)` , `r round(Rewards_hdi_Dist[1,4], digits = 3)`|) with P(Far > Close|data) = `r paste(round(mean(Pabove0_Dist$above0), digits = 3)*100, "%", sep = "")`. This suggests that participants were sensitive to the addition of a reward in this experiment.

In addition to this, the interaction of Distance Type with Gamble Type was in the direction of an asymmetrical reward structure pushing participants to make more optimal decisions with participants standing closer to one of the targets when they opted for an unequal split (mean of `r round(Rewards_hdi_interaction[4,4], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_interaction[4,3], digits = 3)` , `r round(Rewards_hdi_interaction[4,5], digits = 3)`|) than when they had opted for an equal split (mean of `r round(Rewards_hdi_interaction[2,4], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_interaction[2,3], digits = 3)` , `r round(Rewards_hdi_interaction[2,5], digits = 3)`|) with P(Unequal > Equal|data) = `r paste(round(mean(Pabove0_int$above0Far), digits = 3)*100, "%", sep = "")`. 

# Probability Matching Experiment

## Introduction 

In this experiment, we made the probability of the target appearing in one of the two locations higher (80%) than the other (20%). When predicting which of two events is about to occur (for instance, whether a blue or yellow light would illuminate on a particular trial), people tend to match the underlying probability of each event, a tendency called Probability Matching [@Koehler2010; @Goodnow1955; @Vulkan2000ProbMatching]. If event A occurs 80% of the time, and option B only 20% of the time, the best course of action is to predict event A every time, as this would result in an average accuracy rate of 80%. People, in general, tend to instead select each option in proportion to its likelihood of success, yielding an average success rate (in this example) of only 68%. 

Probability matching may be due in part to a misunderstanding of probability [@REIMERS201811], but it has also been argued to reﬂect a reasonable tendency to seek out and exploit patterns in sequential events [@gaissmaier2008smart; @wolford2004searching]. @Gao2015 argue that when a person selects the more likely option on every trial (a strategy known as _Maximising_), they are accepting a loss for a minority of trials (20% in this example). As demonstrated in @KahnemanChoicesValuseFrames, people tend to ﬁnd certain loss very aversive, so a strategy that includes a certain loss may be disregarded as a “solution” to the problem at hand [@Goodnow1955]. The only way to avoid this loss is to make an attempt to detect and exploit any potential pattern. In the context of deciding where to ﬁxate between two potential targets locations, in both @morvan2012human and @clarke2015failure, the probability for each location to contain the target target was 50%. Given the philosophical and experimental arguments described above, participants may react to this choice between equally likely options with idiosyncratic pattern-seeking behavior [@gaissmaier2008smart; @yellott1969probability; @wolford2004searching]. In @clarke2015failure, this may have interfered with participants using information about their own ability to make decisions. Instead, they may have been attempting to ﬁgure out patterns in the sequence of targets to make better guesses about which one was likely to be the target on each trial. Adding in a bias for one option to be more likely may cater to people’s tendency to seek out, and attempt to exploit, patterns. It also breaks the Buridan’s Ass deadlock and makes the decision about which of two goals to attempt easier. In this Experiment, unequal probability was introduced into the paradigm used by @clarke2015failure in order to investigate 1) whether participants would make use of probability information in deciding where to ﬁxate, and 2) whether probability manipulations might facilitate the decision about whether to ﬁxate between two potential target locations or to look directly at one of them. If so, it would suggest that at least part of the reason for the poor decisions observed previously was the (unnatural) balance between the two potentialt argets. Additionally,this experiment will provide a useful replication of both the ﬁxations decision results [@morvan2012human; @clarke2015failure] and the probability matching tendency [@Vulkan2000ProbMatching] in a new context.


## Methods 

### Participants
16 Particiapnts (two Male) were recruited from the University of Aberdeen community with an average age of 22.75 (between 19 and 29). Each participant was reimbursed £10 for their time. 

### Procedure
The experiment followed a similar procedure to the "_Detection Task_" from @clarke2015failure. In the first session, we measured visual acuity which lasted approximately 30 minutes. This was followed by a second session in which the participants performed the actual decision task which lasted approximately 40-50 minutes.

The experiment took place in a darkened room on a desktop computer. An Eyelink 1000 (version 4.594)(SR Research ltd, Mississauga, Ontario, Canada) was used to record eye position at 1000Hz. In each session, a 5 point calibration was carried out with additional calibration and validation sequences prior to each block, and if the participants had broken fixation ten times cummulatively or for five trials in a row. The stimuli were displayed on a CRT monitor (resolution 1920 X 1080 pixels) using Matlab 7.9.0 (R2009b) with Psychtoolbox [@brainard1997psychophysics; @pelli1997videotoolbox] and EyelinkToolbox functions [@cornelissen2002eyelink]. A chin rest with forehead bar was used to ensure participants maintained a viewing distance of  $\approx$ 47cm.

In the first session, participants were instructed to remain ﬁxated at the center and identify a letterthatwouldappearinoneoftwoboxes. Atthestartofeachtrial,participantsﬁxatedacentral black cross on a grayscale background and pressed the spacebar. The cross was ﬂanked by two boxes which were a lighter shade of grey and occupied 1◦ of visual angle. After a stable ﬁxation had been maintained for 700ms, the target would appear in one of the two boxes for 500ms. The target was a white letter, 0.4&deg; of visual angle, drawn using the Sloan font as these letters are generally of equal recognisability at different viewing angles[@sloan1952comparison]. Ten letters were used and one was selected randomly on each trial to be the target letter. Participants were then presented with a screen prompting them to report which letter was presented by clicking on the corresponding character. An illustration of each trial in Session 1 can be seen in Figure \@ref(fig:ProbSession1).

```{r ProbSession1, include=T, echo=F, fig.asp=.45, fig.cap="_The sequence for every trial in Session 1_"}

plt_fig1 <- readPNG("../Figures/Experiment_4_Prob/Part1_Trial.png")
grid.raster(plt_fig1)

```

The boxes were presented on either side of the ﬁxation cross at several different eccentricities (3&deg;,4.3&deg;,5.8&deg;,7.5&deg;,9.3&deg;,11.1&deg;,12.5&deg;,& 13.7&deg;). Each eccentricity was repeated 12 times in a row before moving on to another eccentricity (the order of these sets of 12 eccentricities was random),for a total of 96 trials in a block.

After each block, participants were offered a break before recalibrating and moving onto the next block. Participants completed four of these blocks. The data from this ﬁrst session were used to tailor the separations that would be used for the second session. A “switch-point” was calculated for each participant based on their Session 1 performance. This was then used as an anchor point from which to calculate the other separations to be used in Session 2. The switch-point for each participant was the separation at which the participant was 68.5% accurate. The accuracy level of 68.5% is the mid-point between 55% and 82%; these two values are the points at which participants should switch from ﬁxating the central box to the side box in the Symmetric and Bias conditions, respectively(see below for more details).


```{r ProbSession2, include=T, echo=F, fig.asp=.45, fig.cap="_The sequence for every trial in Session 2_"}

plt_fig2 <- readPNG("../Figures/Experiment_4_Prob/Part2_Trial.png")
grid.raster(plt_fig2)

```

From this point, 6 other separations were calculated $\pm$ 1&deg;, 2&deg;, and 3&deg;. There were two other points included which acted as anchors, one being a very large distance (19.4&deg;), the other very small (1.9&deg;). In Session 2, participants started by ﬁxating a cross that appeared above where the boxes would appear. The cross would appear at the midpoint between the centre box and either the left or right box with an equal probability. Once they had ﬁxated the cross, they were instructed to press the spacebar. After a stable ﬁxation was detected for 700ms, three boxes would be presented (Figure \@ref(fig:ProbSession2)). One box would appear in the centre with the other two spaced equally on either side with separations calculated as above, based on Session 1 performance for that particular participant. Once these boxes had appeared, participants were instructed to ﬁxate one of the three boxes. Participants were told that the target would never appear in the central box, however they could choose to ﬁxate this location. After they had ﬁxated one of the boxes, the letter appeared in one of the side boxes for 500ms. After this, the 10 letter stimuli were presented on screen for them to select which letter they had seen. Each separation appeared 10 times within a block, for a total of 90 trials. The order of separations was randomised (rather than blocked, as it was in Session 1).

Prior to each block, participants were told how likely each box was to contain the target. There were two levels of probability; one in which each box was equally likely to contain the target on each trial (Symmetric), and one in which one side would contain the target 80% of the time (Biased). Each participant took part in both the Symmetric and Biased conditions. Participants completed one condition for 4 blocks before moving on to the other condition. This order was counterbalanced across participants. Additionally, the side that was more likely to contain the target was also counterbalanced, to control for any bias to look to one side over the other.

### Optimal Strategy

The optimal strategy is to ﬁxate whichever of the three boxes maximises expected accuracy. We refer to a participant’s expected accuracy if they had followed the optimal strategy, as “optimal accuracy”.  Using the psychometric curves ﬁt to each participant’s Session 1 data, we can calculate how likely a given person is to detect the target at various distances. This was then used to predict how accurate that same person would be if they had ﬁxated one of the side boxes, or the centre box, by using their average accuracy for a given distance from the left and right box ($Bl$ and $Br$ respectively) in Session 1. The ﬁxation choice that gave the greatest expected accuracy was then selectedastheoptimaldecisionfortherespectiveboxseparation. Asprobabilitywasalsoafactor in this experiment, this had to be accounted for by multiplying the chance of detecting the target in either box by the probability that the box would contain the target ($Pl$ and $Pr$). The formula for expected accuracy is therefore: $(Bl \times Pl) + (Br \times Pr)$. Expected accuracy given optimal ﬁxations differs between the Symmetric and Biased conditions. For example, in the Symmetric condition, participants could expect a 55% success rate if they ﬁxated the optimal location when the targets were far apart. This value comes from assuming they would be $\approx$ 100% accurate for the ﬁxated box, and at chance level for the non-ﬁxated box (10%). This gives us $(1 \times 0.5) + (0.1 \times 0.5) = 55%$. To get the lower limit for the Bias condition, we simply change the chance for each box to contain the target (additionally, we assume that the participant would ﬁxate the most likely box). This gives us $(1 \times 0.8) + (0.1 \times 0.2) = 82%$. This same formula can be used to calculate expected accuracy had the participant ﬁxated the central box. In this case, $Bl = Br$, and so would simply be put into the formula using the appropriate values for $P$, as demonstrated above. 

### Analysis
All analysis for this experiment follow the same procedures as described in previous sections. 

## Results
```{r ProbProp, include = T, echo = F, fig.asp=.45, fig.cap="_These boxplots show the proportion of the time that participants fixated the centre (left panel), most-likely (central panel) and least-likely (right panel) box. In the _Bias_ condition, 'most likely' was the box with and 80% probability of containing the target. In the _Symmetric_ condition, 'most-likely' was whichever side box a given participant had fixated the most. The Close and Far distinction is baseed on when participants should swith from fixating the central box to the side box. Note that for some participants, expected performance differences between the Centre and Side strategies for the closest separation was negligible. However, all participants should have fixated the Side boxes in the Far condition, where the performance advantage of doing so was substantial._"}

plt_prop_fix <- readPNG("../Figures/Experiment_4_Prob/boxes_prop_CML.png")
grid.raster(plt_prop_fix)

```

The choices made by participants on where to look are summarized in Figure \@ref(fig:ProbProp). The optimal strategy in this situation is to ﬁxate the central box when the two sides boxes are near (i.e., when their distance from centre, $\Delta$, is closer than each participant’s switchpoint). When they are far (i.e. delta is greater than the switch point), a participant behaving optimally should ﬁxate either of the side boxes when in the symmetric condition, and the most-likely box when in the biased condition. The ﬁrst panel of Figure \@ref(fig:ProbProp) clearly shows that our participants did not follow this strategy.  However, while adding a bias to the location of the target did not help participants to behave optimally, we can see that it did have an effect on their behavior, as they were much less likely to ﬁxate the central box. Furthermore, when ﬁxating one of the two side squares, they ﬁxated the square that was most likely to contain the target (Figure \@ref(fig:ProbProp), central and right panel) almost all the time

```{r ProbExpAcc, include = T, echo = F, fig.asp = .45, fig.cap = "_These show the expected accuracy for each participant (black lines) plotted against $\\Delta$. The different shaded regions show the minimum (blue) and maximum (red) expected accuracy rates for all participants. $\\Delta$ is the separations at which each participant was tested rescaled to be between 0 and 1_"}
plt_prop_expAcc <- readPNG("../Figures/Experiment_4_Prob/region_performance.png")
grid.raster(plt_prop_expAcc)
```

We can also look at how the choice of where to look inﬂuenced participants’ accuracy. We use the target detection models from Session 1 to calculate the expected accuracy given either (i) an optimal, (ii) counter-optimal, or (iii) the observed strategy. This measure eliminates the variance present in the actual accuracy data due to the random location of the target and participant’s guessing the correct answer by chance. These data are summarised in Figure \@ref(fig:ProbExpAcc). We can see that in the symmetric condition, with the exception of one individual, our participants behaved in a way that gave rise to expected accuracies much closer to the counter-optimal strategy than optimal. Expected accuracy was higher in the biased condition, with four participants reaching optimal performance, with the rest ending up somewhere between the optimal and counter-optimal. 



### Modelling
#### Prior Predictive checks
```{r Prob load data, include = F, echo = F}
# load data
load("../Analyses/Experiment_4_Prob/modelling/BRMS/model_data/df_model")

# load function
source("extract_draws_functions/Prob_draws.R")

n_iter <- 2000
n_warmup <- n_iter/2
```

``` {r Setting Priors, include = F, echo = T}
# setting priors
Rewards_priors <- c(set_prior("student_t(3, 0, .5)", class = "b"),
                    set_prior("student_t(3, 0, .5)", class = "b", coef = "bias_typeSymmetric"),
                    set_prior("student_t(3, 0, .5)", class = "b", coef = "bias_typeSymmetric:dist_typeFar"),
                    set_prior("student_t(3, 0, .5)", class = "b", coef = "dist_typeFar"),
                    set_prior("student_t(3,0,1)", class = "Intercept"))
```

```{r ProbPriors, include = F, echo = T, cache = T}
ProbPriors_model <- brm(Ml_fix ~ (bias_type + dist_type)^2 + (dist_type * bias_type|participant),
                        data = df_model,
                        family = "bernoulli",
                        prior = Rewards_priors,
                        chains = 1,
                        iter =   n_iter,
                        warmup = n_warmup,
                        sample_prior = "only")
```


```{r ProbPriorsPlt, include = T, echo = F}
ProbPriordraws <- Prob_draws(ProbPriors_model)

plt_ProbPrior_interaction <- ProbPriordraws$plt_estimates + 
  scale_x_continuous(expression(paste("Normalised ", Delta))) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  theme_bw() + 
  guides(fill = guide_legend("Bias Type"),
         colour = guide_legend("Bias Type"))

prior_draws <- ProbPriordraws$draws_df %>%
  mutate(prop = boot::inv.logit(estimate)) %>%
  select(-estimate) 

plt_ProbPrior_meDist <- prior_draws %>%
  ggplot(aes(prop,
             colour = Dist_type,
             fill = Dist_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() +  
  theme(axis.title.x = element_blank()) +
  labs(fill = "Distance Type",
       colour = "Distance Type") +
  #       x = "Proportion of Fixations to Most Likely Side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

plt_ProbPrior_meBias <- prior_draws %>%
  ggplot(aes(prop,
             colour = Bias_type,
             fill = Bias_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() + 
  theme(axis.title.x = element_blank()) +
  labs(fill = "Bias Type",
       colour = "Bias Type") +
  #       "Proportion of Fixations to Most Likely Side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

gridExtra::grid.arrange(plt_ProbPrior_meDist, plt_ProbPrior_meBias, plt_ProbPrior_interaction)

```


```{r ProbModel, include = F, echo = T, cache = T}
ProbModel <- brm(Ml_fix ~ (bias_type + dist_type)^2 + (dist_type * bias_type|participant),
                 data = df_model,
                 family = "bernoulli",
                 prior = Rewards_priors,
                 chains = 1,
                 iter = n_iter,
                 warmup = n_warmup)
```


```{r ProbModelPlts, include = T, echo = F}
Probdraws <- Prob_draws(ProbModel)

plt_ProbPost <- Probdraws$plt_estimates + 
  scale_x_continuous(expression(paste("Normalised ", Delta))) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  theme_bw() + 
  guides(fill = guide_legend("Bias Type"),
         colour = guide_legend("Bias Type"))
```


```{r Get Values, include = F, echo = F}
# get draws
draws <- Probdraws$draws_df %>%
  mutate(prop = boot::inv.logit(estimate)) %>%
  select(-estimate)

# HDI_values 
HDI_Bias <- draws %>%
  group_by(Bias_type) %>%
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2],
            med = median(prop))
HDI_Dist <- draws %>% 
  group_by(Dist_type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2],
            med = median(prop))
HDI_interaction <- draws %>% 
  group_by(Bias_type, Dist_type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2],
            med = median(prop))

# Diff values 
Diff_Bias <- draws %>% 
  spread(Bias_type, 
         prop) %>%
  mutate(diff = Bias - Symmetric,
         Bias_bigger = ifelse(diff > 0, 1, 0))
D_bias_lower <- hdci(Diff_Bias$diff)[,1]
D_bias_mu <- mean(Diff_Bias$diff)
D_bias_upper <- hdci(Diff_Bias$diff)[,2]
p_bias_bigger <- mean(Diff_Bias$Bias_bigger)

Diff_Dist <- draws %>% 
  spread(Dist_type, 
         prop) %>%
  mutate(diff = Far - Close,
         Far_bigger = ifelse(diff > 0, 1, 0))
D_dist_lower <- hdci(Diff_Dist$diff)[,1]
D_dist_mu <- mean(Diff_Dist$diff)
D_dist_upper <- hdci(Diff_Dist$diff)[,2]
p_far_bigger <- mean(Diff_Dist$Far_bigger)

Diff_Interaction_compdist <- Diff_Dist %>% 
  group_by(Bias_type) %>% 
  summarise(p_Far_bigger = mean(Far_bigger),
            lower = hdci(diff)[,1],
            mu = mean(diff),
            upper = hdci(diff)[,2],
            med = median(diff))
Diff_Interaction_compbias <- Diff_Bias %>% 
  group_by(Dist_type) %>% 
  summarise(p_Bias_bigger = mean(Bias_bigger),
            lower = hdci(diff)[,1],
            mu = mean(diff),
            upper = hdci(diff)[,2],
            med = median(diff))

```

The results of this model suggest that our participants were sensitive to the probability information (see above figure). In the Biased condition, the average participant fixated the most likely target `r paste(round(HDI_Bias[1,3], digits = 3)*100, "%", sep = "")` of the time (95% HDPI of |`r paste(round(HDI_Bias[1,2], digits = 3)*100, "%", sep = "")`, `r paste(round(HDI_Bias[1,4], digits = 3)*100, "%", sep = "")`|), compared to `r paste(round(HDI_Bias[2,3], digits = 3)*100, "%", sep = "")` (95% HPDI of |`r paste(round(HDI_Bias[2,2], digits = 3)*100, "%", sep = "")`,`r paste(round(HDI_Bias[2,4], digits = 3)*100, "%", sep = "")`|) in the symmetric condition. The width of these intervals reflects a high degree of uncertainty in fixed effects, due to the range of behaviours exhibited by participants. None-the-less, the HPDI on the difference between these two conditions, |`r round(D_bias_lower, digits = 3)*100`%, `r round(D_bias_upper, digits = 3)*100`%| is largely positive and we can be reasonably confident (P(difference > 0 | data ) =  `r round(p_bias_bigger, digits = 3)*100`%) that the most-likely target is fixated more frequently in the biased condition. The distance between the square targets did not appear to have any consistent effect in the Symmetric condition, however there was a small decrease in fixations towards the “most likely” box when the boxes were far apart in the bias condition (dropping from `r round(HDI_interaction[1,4], digits = 3)*100`%, 95% HPDI of |`r round(HDI_interaction[1,3], digits = 3)*100`%,`r round(HDI_interaction[1,5], digits = 3)*100`%| to `r round(HDI_interaction[2,4], digits = 3)*100`%, 95% HPDI of |`r round(HDI_interaction[2,3], digits = 3)`%,`r round(HDI_interaction[2,5], digits = 3)*100`%|). As such, the difference between the conditions were much more pronounced in the close condition (P(Bias > Symmetric|data) = `r round(Diff_Interaction_compbias[1,2], digits = 3)*100`%) than in the far condition (P(Bias > Symmetric|data) = `r round(Diff_Interaction_compbias[2,2], digits = 3)*100`%). The width of these intervals reflects the range of performance that was exhibited by participants. However, this does show that participants generally made us of this probability information in order to decide where to fixate. 

Note: Do we want the stuff about side fixations in here?
```{r ProbModelOutputs, include = T, echo = F, fig.cap = "_Posterior predictions for the probability matching experiment_"}
# Proportion fixations to most likely side 
plt_me_Bias <- draws %>% 
  ggplot(aes(prop,
             colour = Bias_type,
             fill = Bias_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() + 
  theme(axis.title.x = element_blank()) +
  labs(fill = "Bias Type",
       colour = "Bias Type") +
  #       "Proportion of Fixations to Most Likely Side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

plt_me_Dist <- draws %>% 
  ggplot(aes(prop,
             colour = Dist_type,
             fill = Dist_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() +  
  theme(axis.title.x = element_blank()) +
  labs(fill = "Distance Type",
       colour = "Distance Type") +
  #       x = "Proportion of Fixations to Most Likely Side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))


gridExtra::grid.arrange(plt_me_Dist, plt_me_Bias, plt_ProbPost)
```


## Discussion 

# Bibliography
