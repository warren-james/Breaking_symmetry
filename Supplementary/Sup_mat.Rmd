---
title: "Supplementary Material for Asymmetry paper"
output: 
  bookdown::pdf_document2:
    #theme: journal
    toc: true
    keep_tex: false
bibliography: ["Literature.bib"]
biblio-style: "apalike"
link-citations: true
---

---
title: "Supplementary Material For Asymmetry paper"
output: html_document
---


```{r setup, include=T, message=F, warning=F, echo= F}
knitr::opts_chunk$set(fig.align = "center")
# library 
library(tidybayes)
library(tidyverse)
library(brms)
library(png)
library(grid)
library(gridExtra)
library(psyphy)

# functions 
# function to extract a legend that is shared accross plots
g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}
```

These supplementary materials present full details of all R packages used in the analysis; a detailed explanation of our simulation-based power analysis; and Bayesian models of the results discussed in the main manuscript. In addition, we also present details a further, related experiment using eye-tracking and a probability matching paradigm. 

Please see the accompanying source Rmd file for the code. 

# Session Info

```{r, include=T, echo=T}
sessionInfo()
```

``` {r, echo = F, include = F, results = "hide"}
options(mc.cores = parallel::detectCores())
```
# Power Analysis

Our power analysis made use of simulations and resampling approaches based on data that we previously collected. Resampling was carried out in order to establish if adding more participants to our sample sizes would have lead to a reduction in uncertainty around our estimates. The experiments were all interested in the extent to which a distribution of standing position shifted with the introduction of a new factor. As such, one power analysis was carried out to determine what sort of sample size would be sufficient to detect a small shift in the mean of this distribution.

We started by fitting a a beta distribution to each participant's data from the _Throwing Experiment_ in @clarke2015failure using the `fitdistrplus` package (as can be seen in Figure \@ref(fig:drawdistributions). In the original data, normalised standing position was coded $\phi \in [0,1]$ with $0$ indicating a central standing position, and $1$ indicating the participant stood by either of the two hoops. As we are _breaking the symmetry_ between the two targets in this new experiment, we will now treat standing position as $\phi \in (0, 1)$ with $0$ and $1$ representing the positions of the large and small hoop respectively. The central midpoint is now represented as $\phi = 0.5$. 

This is directly linked to the idea behind the Hoop Size experiment in which we aimed to observe whether participants would shift towards one side when presented with hoops of different sizes. However, this can be applied to the other experiments in that they are also looking at how the distribution shifts given a new factor in the experiment. 

```{r drawdistributions, include = T, echo = F, results = "hide", fig.height = 3, fig.cap="These plots show the various distributions that were fit to each participant. These were then used to draw random samples from in order to simulate participants shifting towards one target."}
# load("../power/Hoop_size/scratch/sampled_data")
load("../power/Hoop_size/scratch/df_part2")

# functions for getting parameters from a beta dist
mu_beta <- function(a, b){
  a/(a + b)
}
phi_beta <- function(a, b){
  1/((a*b)/(((a+b)^2)*(a+b+1)))
}
var_beta <- function(a, b){
  (a*b)/(((a+b)^2)*(a+b+1))
}

# get shape parameters 
get_shape_beta <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

# sort out beta shape
shape_1 <- fitdistrplus::fitdist(df_part2$beta_pos, "beta")
mu1 <- mu_beta(shape_1$estimate[1], shape_1$estimate[2])
var1 <- var_beta(shape_1$estimate[1], shape_1$estimate[2])
shape_1 <- get_shape_beta(mu1, var1)
mu2 <- seq(0.05, .2, 0.05)
x <- seq(0.02,1-0.02,0.02)

# draw these 
df_distributions <- tibble(x_vals = rep(x, length(c(0,mu2))),
                           base = mu1,
                           diff = rep(c(0,mu2), each = length(x)),
                           var = var1,
                           alpha = get_shape_beta(base + diff, var)$alpha,
                           beta = get_shape_beta(base + diff, var)$beta,
                           est_mu = mu_beta(alpha, beta),
                           est_var = var_beta(alpha, beta),
                           p = dbeta(x_vals, alpha, beta))

plt_both <- df_distributions %>% 
  mutate(diff = as.factor(diff)) %>%
  ggplot(aes(x_vals, p)) + 
  geom_histogram(data = df_part2, 
                 aes(x = (norm_pos + 1)/2,
                     y = ..density..),
                 binwidth = .1,
                 colour = "blue",
                 fill = "blue",
                 alpha = .2) + 
  geom_line(aes(colour = diff)) + 
  theme_bw() + 
  see::scale_color_flat() +
  guides(colour = guide_legend("Difference")) +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        legend.position = "bottom")

plt_dists <- df_distributions %>% 
  mutate(diff = as.factor(diff)) %>% 
  ggplot(aes(x_vals, p)) + 
  geom_line(aes(colour = diff)) + 
  theme_bw() + 
  see::scale_color_flat() +
  scale_x_continuous("standing position") + 
  guides(colour = guide_legend("Difference")) + 
  theme(
    axis.title.y = element_blank(),
    legend.title = element_blank(),
    legend.position = "right"
  )

plt_hist <- df_distributions %>% 
  filter(diff == 0) %>% 
  mutate(diff = as.factor(diff)) %>% 
  ggplot(aes(x_vals, p)) + 
  geom_line(aes(colour = diff)) + 
  see::scale_color_flat() + 
  theme_bw() + 
  geom_histogram(data = df_part2, 
                 aes(x = (norm_pos + 1)/2, y = ..density..),
                 binwidth = .1,
                 colour = "blue", 
                 fill = "blue",
                 alpha = .2) + 
  scale_x_continuous("standing position") +
  theme(
    legend.position = "none",
    axis.title.y = element_blank()
  )

# gridExtra::grid.arrange(plt_hist, plt_dists, ncol = 2)


# Same as above but for each participant
# Need to sample each participant and get parameters for them 
temp <- df_part2 %>% 
  group_by(Participant) %>% 
  summarise(a = fitdistrplus::fitdist(beta_pos, "beta")$estimate[1],
            b = fitdistrplus::fitdist(beta_pos, "beta")$estimate[2])
mus <- seq(0, .2, .05)

df_distributions <- tibble(participant = rep(temp$Participant, each = length(mus)),
                           og_as = rep(temp$a, each = length(mus)),
                           og_bs = rep(temp$b, each = length(mus)),
                           og_mu = mu_beta(og_as, og_bs),
                           og_var = var_beta(og_as, og_bs),
                           increase = rep(mus, length(temp$a))) %>% 
  mutate(mu = og_mu + increase,
         var = og_var, 
         alpha = get_shape_beta(mu, var)$alpha,
         beta = get_shape_beta(mu, var)$beta) %>% 
  select(participant, increase, mu, var, alpha, beta)

# make some plots
df_dists <- tibble(Participant = rep(unique(df_distributions$participant), each = length(x) * length(mus)),
                   x_vals = rep(x, length(mus) *length(unique(df_distributions$participant))),
                   increase = rep(rep(mus, each = length(x)), length(unique(df_distributions$participant))),
                   alpha = rep(unique(df_distributions$alpha), each = length(x)),
                   beta = rep(unique(df_distributions$beta), each = length(x))) %>% 
  mutate(increase = as.factor(increase)) %>% 
  group_by(Participant, increase, x_vals) %>% 
  mutate(y = dbeta(x_vals, alpha, beta)) %>% 
  filter(increase %in% c(0, .05))


# make plot 
plt_dists_wP <- df_part2 %>%
  ggplot(aes(beta_pos)) + 
  geom_histogram(aes(y = ..density..),
                 binwidth = .1
  ) + 
  geom_line(data = df_dists,
            aes(x_vals, y,
                colour = increase)) + 
  see::scale_color_flat() + 
  scale_x_continuous(expression(Phi), breaks = c(0, 0.5, 1)) +
  theme_bw() + 
  theme(strip.text.x = element_blank()) + 
  facet_wrap(~Participant, scales = "free")
plt_dists_wP
```

The fit with the empirical data is reasonable, although we under-estimate the frequency of standing positions $\phi\approx0.5$. In this case 0.5 is considered the central point with 0 and 1 representing the left and right hoop respectively. We assumed that by introducing a difference in size between the two hoops that this distribution would shift towards the smaller hoop so as to balance their chances of success. We assumed that the difference in hoop size would cause participants to stand slightly closer to the small hoop, in this case the shift in mean would be +0.05. 

```{r Hoop Size resampling, include = T, echo = F, cache = T}

# setup loop
diff_sample <- seq(.05, .2, .05)
n_iter <- 5000
n_subs <- seq(3, 25, 1)
n_trials <- 72
n <- length(diff_sample) * n_iter * length(n_subs)
refresh <- n_iter/100

df_sample <- data.table::data.table(iter = rep(0, n),
                                    increase = rep(0, n),
                                    n_subs = rep(0, n),
                                    n_trial = rep(0, n),
                                    base = rep(0, n),
                                    comp = rep(0, n))

# loop 
count <- 1 

for(ii in 1:n_iter){
  for(inc in diff_sample){
    for(subN in n_subs){
      subs <- sample(unique(df_distributions$participant), subN, replace = T)
      baseline <- c()
      comparison <- c()
      for(sub_iter in 1:length(subs)){
        base_a <- df_distributions$alpha[df_distributions$participant == subs[sub_iter] &
                                           df_distributions$increase == 0]
        base_b <- df_distributions$beta[df_distributions$participant == subs[sub_iter] &
                                          df_distributions$increase == 0]
        comp_a <- df_distributions$alpha[df_distributions$participant == subs[sub_iter] &
                                           df_distributions$increase == inc]
        comp_b <- df_distributions$beta[df_distributions$participant == subs[sub_iter] &
                                          df_distributions$increase == inc]
        baseline <- c(baseline, rbeta(n_trials, base_a, base_b))
        comparison <- c(comparison, rbeta(n_trials, comp_a, comp_b))
      }
      df_sample[count, iter := ii]
      df_sample[count, increase := inc]
      df_sample[count, n_subs := subN]
      df_sample[count, n_trial := n_trials]
      df_sample[count, base := mean(baseline)]
      df_sample[count, comp := mean(comparison)]
      count <- count + 1
    }
  }
  # if(ii %% refresh == 0){
  #   print(paste((ii/n_iter)*100, "%", sep = ""))
  # }
}

```

We can now these use distributions to simulate experiments with $N = 3\ldots 24$ participants and 72 trials. Figure \@ref(fig:HDIHoopSize) shows the uncertainty surrounding the mean estimate for the smallest difference tested (5\%). After 15 participants, the uncertainty surrounding the estimate appears to plateau which demonstrates that the sample size of 21 was sufficient to detect an effect of this size. 

```{r HDIHoopSize, include = T, echo = F, warning = F, fig.height = 3, fig.cap="This figure shows how the 95\\% HDI around the mean difference for the smallest difference simulated (5\\%) changed with a larger sample size"}

df_sample %>% 
  filter(increase == 0.05) %>% 
  group_by(n_subs) %>% 
  mutate(diff = comp - base) %>% 
  summarise(mu = mean(diff), 
            lower = hdi(diff)[1],
            upper = hdi(diff)[2]) %>% 
  ggplot(aes(n_subs, mu)) + 
  geom_line() + 
  geom_ribbon(aes(ymin = lower,
                  ymax = upper),
              alpha = .3) + 
  scale_x_continuous("Sample Size") + 
  theme_bw() + 
  theme(axis.title.y = element_blank())

```


# Additional Analyses

In the analyses below, we use multi-level Bayesian beta regression to model the change in standing position $\phi$ when we 1) use unequal hoop sizes, 2) remove the need to chose between hoops, and 3) provide unequal rewards. 

## Experiment 1: Hoop Size

```{r load data and source function Hoop Size, echo = F, include = F}
# load model data 
load("../Analyses/Experiment_1_Hoop_size/data/model_data")

# get function for extracting draws 
source("extract_draws_functions/Hoop_size_draws.R")

load("../Analyses/Experiment_1_Hoop_size/scratch/beanbagdat")
```

```{r HoopSizeSession1, echo = F, warning = F, message = F, fig.height = 3, fig.cap = "Each facet shows how a participant's accuracy dropped as a function of distance for both hoop sizes that were used in the experiment. The x-axis has been normalised so that 1 represents the furthest distance tested."}
beanbagdat %>% 
  mutate(delta = slab/max(slab),
         hoop_size = factor(hoop_size, labels = c("Large", "Small"))) %>% 
  ggplot(aes(delta, acc, colour = hoop_size)) + 
  geom_point() + 
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"),
              se = F,
              fullrange = T) + 
  facet_wrap(~participant, ncol = 7) + 
  theme_bw() + 
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")", sep = "")),
                     breaks = seq(0,1,.5)) + 
  scale_y_continuous("Accuracy", labels = scales::percent_format(accuracy = 1)) + 
  guides(colour = guide_legend("Hoop Size")) +
  theme(strip.text.x = element_blank())

```
Figure \@ref(fig:HoopSizeFacets) shows the data for each or the participants. Each plot shows the standing position on every trial for each of the standardised distances the participants were tested at. The dots are set to be somewhat translucent so darker regions demonstrate the participant stood in this location more often. Anything above the dashed line demonstrates the participant shifting towards the smaller hoop, while below represents the participant opting to be closer to the big hoop.

```{r HoopSizeFacets, include=T, echo=F, fig.width = 10, fig.height = 8, fig.cap = "Each facet of this plot shows were each participant stood on each trial. The x-axis shows the different distances that participants were tested at with 0 being the point at which they were closest 50\\% from the centre. The y-axis shows the normalised standing position between the Big Hoop and the Small Hoop."}
load("../Analyses/Experiment_1_Hoop_size/scratch/df_part2_norm")
df_hoopsize <- norm_dat
rm(norm_dat)

# process data 
df_hoopsize %>% 
  group_by(participant) %>% 
  mutate(temp = as.numeric(as.factor(hoop_pos)),
         slab_measures = factor(temp, labels = c("~90%", "~50% - 1", "~50%", "~50% + 1", "~50% + 2", "~10%"))) %>% 
  select(-temp) %>%
  ungroup() %>% 
  filter(abs(norm_dist) <= 1) %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(hoop_pos))-> df_hoopsize

# make plot
df_hoopsize %>% 
  ggplot(aes(overall_hoop_pos, norm_dist)) + 
  geom_jitter(alpha = .2,
              width = .2,
              height = .2) +
  facet_wrap(~participant, ncol = 3) + 
  geom_hline(yintercept = 0, 
             linetype = "dashed") +
  scale_y_continuous("", 
                     breaks = c(-1, 0, 1),
                     labels = c("Big Hoop", "Centre", "Small Hoop")) +
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")")),
                     expand = c(0.1,0.1),
                     breaks = unique(df_hoopsize$overall_hoop_pos),
                     labels = c("-1", "0", "+1", "+2", "F", "C")) +
  theme_bw() + 
  theme(strip.text.x = element_blank(),
        panel.grid.minor = element_blank())

```



The data from this experiment were analysed using a Bayesian beta regression. The recorded data for standing positions were transformed to be between 0 and 1, with 0 representing the larger hoop and 1 representing the smaller hoop. Therefore, the central point would be 0.5, and meaning anything above this value would demonstrate a shift in participant’s behavior away from the mid-point and towards the small hoop. The model included normalised hoop delta as a predictor to see how participants changed position with an increasing distance. This was also entered as a random effect by participant. 


### Modelling

For all models, we opted to use priors that were weakly informative. The chosen priors made the weak assumption that there was no effect of the experimental manipulations introduced in this series of experiments. 

``` {r HS_Prior, include = T, echo = F}
HS_prior <- c(set_prior("student_t(3, 0, 1)",
                        coef = "norm_hoop_pos",
                        class = "b"),
              set_prior("student_t(3,0,1)",
                        class = "Intercept"))
HS_iter <- 5000
HS_control <- list(adapt_delta = .95, max_treedepth = 15)
```

The models themselves looked as follows with the prior predictive checks having the additional "sample_prior" argument set to "_only_": 

```{r run-model-hoop-size-priors, echo = F, include = F, message = F, warning = F, results = "hide", cache = TRUE}
# run model
Hoop_size_priors <- brm(norm_dist2 ~ norm_hoop_pos + (norm_hoop_pos|participant), 
                        data = model_data,
                        family = "beta",
                        prior = HS_prior,
                        cores = 2,
                        chains = 2,
                        iter = HS_iter,
                        warmup = HS_iter/2,
                        control = HS_control,
                        sample_prior = "only")
```

```{r HoopSizePriors, include = F, echo = F, warning = F, fig.width = 3, fig.cap="Prior predictions for the Hoop Size experiment"}
# need to make a dataset
draws <- HS_draw_post(Hoop_size_priors, model_data)

plt_prior_hoopsize <- draws$plt_estimates + 
  scale_x_continuous(expression(Phi), 
                     breaks = c(0,.5,1),
                     labels = c("Big Hoop", "Centre", "Small Hoop")) + 
  theme(legend.position = "bottom")

```


```{r run-model-Hoop-size, include = T, echo = T, message = F, warning = F, results = "hide", cache = TRUE}
# run model
Hoop_size_m1 <- brm(norm_dist2 ~ norm_hoop_pos + (norm_hoop_pos|participant), 
                    data = model_data,
                    family = "beta",
                    prior = HS_prior,
                    cores = 2,
                    chains = 2,
                    iter = HS_iter,
                    warmup = HS_iter/2,
                    control = HS_control)

```

```{r Extract Draws, include = F, echo = F, cache=TRUE}
# get draws
draws <- HS_draw_post(Hoop_size_m1, model_data)
```




```{r Sort Hdis, include = F, echo = F}
p_above_5 <- draws$Prop_above.5
HDI_all <- draws$draws_HDI[[2]]
```


```{r PltHoopSizeEstimates, include = F, echo = F, fig.asp=.45, fig.cap="Posterior predictions for the mean standing position."}
plt_post_hoopsize <- draws$plt_estimates + 
  scale_x_continuous(expression(Phi),
                     limits = c(0,1),
                     breaks = c(0,.5,1),
                     labels = c("Big Hoop", "Centre", "Small Hoop")) + 
  theme(legend.position = "bottom")
```

``` {r HS_results, include = T, echo = F}
# Show fixed effects 
summary(Hoop_size_m1)$fixed
```

The model results confirmed that participants in general had a bias towards standing closer to the small hoop (mean of `r round(HDI_all$mean, digits = 3)`, 95% HDPI of |`r round(HDI_all$lower, digits = 3)`, `r round(HDI_all$upper, digits = 3)`|). We can be reasonably confident about this results as the p(x > 0.5|data) = `r paste(round(p_above_5$above0_5, digits = 3)*100, "%", sep = "")`. This can be seen in the posterior in the right hand plot of Figure \@ref(fig:PltHoopSize). Also, note that distance did not appear to have an effect on position (i.e., participants were generally biased slightly towards the smaller hoop).

``` {r PltHoopSize, include = T, echo = F, fig.asp = .45, fig.cap="Figure on the left shows the Prior predictions. The figure on the right shows the Posterior predictions once the model had been conditioned on the data"}
# get legend 
HoopSizeLegend <- g_legend(plt_prior_hoopsize)

grid.arrange(arrangeGrob(plt_prior_hoopsize + theme(legend.position = "none"), plt_post_hoopsize + theme(legend.position = "none"), ncol = 2),
             HoopSizeLegend, heights = c(10,1))
```

### Raw Accuracy 
```{r HoopSizeRawAcc, include=T, echo=F, fig.width = 10, fig.height = 5, message = F, fig.cap = "This figure shows the average accuracy participants achieved at each distance. Each line represents a participant"}
# make plot
df_meanhoop <- df_hoopsize %>% 
  group_by(slab_measures) %>% 
  summarise(mean_pos = mean(overall_hoop_pos))

df_hoopsize %>% 
  group_by(participant, slab_measures, overall_hoop_pos) %>% 
  summarise(mu = mean(accuracy)) %>% 
  ggplot(aes(overall_hoop_pos, mu)) + 
  geom_path(aes(group = participant),
            alpha = .3) + 
  theme_bw() + 
  scale_y_continuous("Expected Accuracy", labels = scales::percent_format(accuracy = 1)) +
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")", sep = "")),
                     breaks = unique(df_meanhoop$mean_pos),
                     labels = c("C", "-1", "0", "+1", "+2", "F")) +
  theme(axis.text = element_text(size = 12),
        axis.title.y = element_text(size = 12),
        axis.title.x = element_text(size = 12),
        panel.grid.minor = element_blank())

```

## Experiment 2: Two Throws

```{r Two Throws: load data, include = F, echo = F}
load("../Analyses/Experiment_2_Two_throw/scratch/model_data_pos")

# rename some variables for legibility
model_data_pos <- model_data_pos %>% 
  mutate(Num_throws = ifelse(Num_throws == "One-Throw", "One", "Two"))

load("../Analyses/Experiment_2_Two_throw/scratch/df_part1")

source("extract_draws_functions/Two_throw_draws_noint.R")
```

```{r TwoThrowSession1, include = T, echo = F, warning = F, fig.width = 10, fig.height = 5, message = F, fig.cap = "Each facet shows how a participant's accuracy dropped as a function of distance. The x-axis has been normalised so 1 represents the furthest range tested."}
df_part1 %>% 
  mutate(delta = slab/max(slab)) %>% 
  ggplot(aes(delta, acc)) + 
  geom_point() + 
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              se = F) + 
  scale_y_continuous("Accuracy", labels = scales::percent_format(accuracy = 1)) +
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")", sep = ""))) +
  facet_wrap(~participant, ncol = 6) + 
  theme_bw() + 
  theme(strip.text.x = element_blank())

```
Figure \@ref(fig:TwoThrowFacets) shows the individual data for each of the participants in this experiment. Each plot shows where the participants stood on each trial for each of the distances they were tested at with the different conditions being colour coded. 

``` {r TwoThrowFacets, include = T, echo = F, fig.width = 10, fig.cap = "Each facet of this plot shows the stnading position on each trial and condition for each participant. The x-axis shows the different distances that participants were tested at with 0 being the point at which they were closest 50\\% from the centre. The y-axis shows the normalised standing position (between the centre point and one of the two side hoops)."}
# process 
df_TwoThrow <- model_data_pos %>% 
  group_by(Participant) %>%
  mutate(temp = as.numeric(as.factor(HoopDelta)),
         slab_measures = factor(temp, labels = c("~90%", "~50% - 1", "~50%", "~50% + 1", "~50% + 2", "~10%"))) %>% 
  select(-temp) %>% 
  ungroup() %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(HoopDelta))

# make plot 
df_TwoThrow %>% 
  ggplot(aes(overall_hoop_pos, abspos,
             colour = Num_throws)) + 
  geom_jitter(alpha = .2,
              width = .2) + 
  facet_wrap(~Participant, ncol = 6) + 
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")")),
                     breaks = unique(df_TwoThrow$overall_hoop_pos),
                     labels = c("C", "+1","-1", "+2", "0", "F")) +
  scale_y_continuous("",
                     breaks = c(0,1),
                     labels = c("Centre", "Side")) +
  see::scale_color_flat() +
  theme_bw() + 
  guides(fill = guide_legend("No. Throws")) +
  theme(strip.text.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(size = 8),
        legend.position = "bottom")


```

A Bayesian Beta regression was carried out to investigate whether participants performed the task in a more optimal way when they were given the chance to attempt to throw at both targets. The predicted value was the Normalised standing position with 0 being central and 1 being next to one of the hoops. The predictors were the normalised distance of the hoops from the centre (norm_delta), the number of throws participants had (Num_throws), and the interactions between these two terms.

### Modelling 
We first carry out a prior predictive check to confirm that our choice of priors are reasonable. The model was specified as follows with the prior predictive checks having the additional "sample_prior" argument set to "_only_".

```{r TwoThrowsParameters, include = F, echo = F}
TT_prior <- c(
  set_prior("student_t(4,0,.8)", class = "b", coef = "norm_delta"),
  set_prior("student_t(4,0,.8)", class = "b", coef = "norm_delta:Num_throwsTwo"),
  set_prior("student_t(4,0,.8)", class = "b", coef = "Num_throwsTwo"),
  set_prior("student_t(4,0,.8)", class = "b", coef = "Num_throwsOne"))

TT_iter = 10000
TT_control = list(adapt_delta = .99, max_treedepth = 15)

```

```{r Two_Throws_Priors_only_model, include = F, echo = F, message = F, warning = F, results = "hide", cache = TRUE}
Two_throw_priors <- brm(
  abspos ~ 0 + norm_delta * Num_throws + (0 + norm_delta * Num_throws|Participant),
  family = "beta",
  data = model_data_pos,
  prior = TT_prior,
  cores = 1,
  chains = 1,
  iter = TT_iter,
  warmup = TT_iter/2,
  control = TT_control,
  sample_prior = "only")
```

```{r PltTwoThrowPriors_setup, include = F, echo = F}
# get draws 
draws_df <- draw_post_delta(Two_throw_priors, model_data_pos)

priors_plt <- draws_df$Draws_df %>%
  mutate(prop = boot::inv.logit(estimate),
         Dist_type = factor(Dist_type, c("Close", "Mid", "Far"))) %>% 
  ggplot(aes(prop,colour = Num_throws, fill = Num_throws)) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  scale_x_continuous(expression(Phi),
                     limits = c(-.1,1.1),
                     breaks = seq(0,1,.25)) +
  geom_density(kernel = "biweight", alpha = .3) + 
  facet_wrap(~Dist_type) + 
  theme_bw() + 
  guides(fill = guide_legend(title = "No. Throws"),
         colour = guide_legend(title = "No. Throws")) +
  theme(legend.position = "right")

```

Next, we train the model on the data and plot the posterior distributions. 

```{r Two-throws modelling, include = T, echo = T, message = F, warning = F, results = "hide", cache = TRUE}

Two_throw_m1 <- brm(
  abspos ~ 0 + norm_delta * Num_throws + (0 + norm_delta * Num_throws|Participant),
  family = "beta",
  data = model_data_pos,
  prior = TT_prior,
  cores = 1,
  chains = 1,
  iter = TT_iter,
  warmup = TT_iter/2,
  control = TT_control)

```

```{r, echo = F, include = T}
save(Two_throw_m1, file = "TTmodel")
summary(Two_throw_m1)$fixed

```

As can be seen from the summary output above, $\hat{R} \approx 1$.

```{r PltTwoThrowEstimates, include = F, echo = F}
# get draws
draws <- draw_post_delta(Two_throw_m1, model_data_pos)
# example distributions 
plt_examples <- draws$Plots[1][[1]][["data"]] %>% 
  mutate(Dist_Type = factor(Dist_type, c("Close", "Mid", "Far"))) %>% 
  ggplot(aes(prop, 
             colour = Num_throws,
             fill = Num_throws)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  facet_wrap(~Dist_Type) + 
  theme_bw() + 
  scale_x_continuous(expression(Phi),
                     limits = c(-.1,1.1),
                     breaks = seq(0,1,.25)) + 
  coord_cartesian(expand = F) + 
  guides(fill = guide_legend(title = "No. Throws"),
         colour = guide_legend(title = "No. Throws")) +
  theme(legend.position = "right")

# plt_difference 
plt_difference <- draws$Plots[3][[1]][["data"]] %>%
  mutate(Dist_type = as.factor(Dist_type)) %>%
  mutate(Dist_type = factor(Dist_type, c("Close", "Mid", "Far"))) %>% 
  ggplot(aes(diff,
             colour = Dist_type,
             fill = Dist_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() + 
  geom_vline(xintercept = 0,
             linetype = "dashed") +
  # see::scale_color_flat() + 
  # see::scale_fill_flat() + 
  scale_fill_viridis_d() + 
  scale_colour_viridis_d() +
  scale_x_continuous("Difference: One throw - Two throw") +
  guides(fill = guide_legend(title = "Distance"),
         colour = guide_legend(title = "Distance"))

# show this 
# gridExtra::grid.arrange(plt_examples, plt_difference)
```

```{r TwoThrowPlts, include = T, echo = F, fig.height = 3, fig.cap = "The top panel shows the Prior preditcions for the Two Throw experiment. The bottom panel shows the Posterior predictions for the Two Throw expeirment."}
# get legend 
TwoThrowLegend <- g_legend(priors_plt)

grid.arrange(arrangeGrob(priors_plt + theme(legend.position = "none"), plt_examples + theme(legend.position = "none")),
             TwoThrowLegend, ncol = 2, widths=c(6, 1))


```

```{r TwoThrowDifference, include = T, echo = F, fig.height = 3, fig.cap = "This figure shows the difference in the estimates for the Two Throw experiment. This is the diffrence between the red and blue distributions see in the Figure `@ref(fig:TwoThrowPlts)` with a bigger number representing the One Throw condition have a larger mean than the Two Throw condition."}
plt_difference
```

```{r Sort HDI, include = F, echo = F, message = F}
# get overall 
HDI_two_throw_overall_diff <- draws$HDIs[[2]]
HDI_two_throw_mean_pos_overall <- draws$HDIs[[1]]
mu <- HDI_two_throw_mean_pos_overall$mean
lower <- HDI_two_throw_mean_pos_overall$lower
upper <- HDI_two_throw_mean_pos_overall$upper

# get the closest 
HDI_tt_close <- draws$HDIs[[3]] %>% 
  filter(Dist_type == "Close")
mu_close <- HDI_tt_close$mean
lower_close <- HDI_tt_close$lower
upper_close <- HDI_tt_close$upper

HDI_tt_diff <- draws$HDIs[[4]]

P_close <- draws$Above_0[[2]] %>% 
  filter(Dist_type == "Close")
```



The analysis suggested that there was an overall greater tendency for participants in the One-throw condition to stand further from the centre (mean of `r round(mu[1], digits = 3)`, 95% HPDI of |`r round(lower[1], digits = 3)` , `r round(upper[1], digits = 3)`|) than when they were in the Two-throw condition (mean of `r round(mu[2], digits = 3)`, 95% HPDI of |`r round(lower[2], digits = 3)` , `r round(upper[2], digits = 3)`|) with P(One-throw > Two-throw|data) = `r paste(round(draws$Above_0[[1]], digits = 3)*100, "%", sep = "")`. 

This effect was the strongest for the closest separation which reflects the larger amount of variation in standing position with distance in the Two-throw condition (Figure \@ref(fig:TwoThrowPlts). In general, when in the One-throw condition, participants stood further from the centre (mean of `r round(mu_close[1], digits = 3)`, 95% HPDI of |`r round(lower_close[1], digits = 3)` , `r round(upper_close[1], digits = 3)`|) than in the the Two-throw condition (mean of `r round(mu_close[1], digits = 3)`, 95% HPDI of |`r round(lower_close[1], digits = 3)` , `r round(upper_close[1], digits = 3)`|) with P(One-throw > Two_throw|data) = `r paste(P_close$above0*100, "%", sep = "")`. However, as can be seen in \@ref(fig:TwoThrowPlts), the difference is generally small which and consistent across all distances. This means that when participants were given the opportunity to throw to both hoops (i.e., in the Two throw condition), they were still sub-optimal in their performance.


### Raw Accuracy 
```{r TwoThrowRawAcc, include=T, echo=F, message = F, fig.height = 3, fig.cap = "This figure shows the average accuracy participants achieved at each distance. Each line represents a participant The facets represent the two different conditions."}
load("../Analyses/Experiment_2_Two_throw/scratch/df_wit_acc")
# Load data with accuracy
df_new <- df_new %>% 
  select(Participant, Accuracy, Trial.no., Num_throws, HoopDelta)%>%
  ungroup() %>% 
  group_by(Participant) %>% 
  mutate(slab_measures = factor(HoopDelta, labels = c("C", "-1", "0", "+1", "+2", "F")))

# make plot
df_meanhoop <- df_new  %>% 
  ungroup() %>% 
  group_by(slab_measures) %>% 
  summarise(mean_pos = mean(HoopDelta))

df_new <- df_new %>% 
  merge(df_meanhoop)

df_new %>% 
  group_by(Participant, slab_measures, mean_pos, Num_throws) %>% 
  summarise(mu = mean(Accuracy)) %>% 
  ggplot(aes(mean_pos, mu)) + 
  geom_path(aes(group = Participant),
            alpha = .3) + 
  theme_bw() + 
  scale_y_continuous("Expected Accuracy", labels = scales::percent_format(accuracy = 1)) +
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")", sep = "")),
                     breaks = unique(df_meanhoop$mean_pos),
                     labels = c("C", "-1", "0", "+1", "+2", "F")) +
  facet_wrap(~Num_throws) +
  theme(axis.text = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        axis.title.x = element_text(size = 8),
        panel.grid.minor = element_blank())

```



## Experiment 3: Reward
``` {r RewardLoadData, include = F, echo = F, warning = F}
load("../Analyses/Experiment_3_Unequal_Reward/scratch/data/model_data")

source("extract_draws_functions/Reward_draws_noint.R")

load("../Analyses/Experiment_3_Unequal_Reward/scratch/data/df_part1")
```

``` {r RewardsSession1, include = T, echo = F, warning = F, fig.height = 3, fig.cap = "Each facet shows how accuracy changed over distance for each pariticipant. The x-axis has been normalised with 1 being the furthest"}
df_part1 %>% 
  mutate(Delta = Slab/max(Slab)) %>%
  ggplot(aes(Delta, Accuracy)) + 
  geom_point() + 
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"),
              se = F,
              colour = "black") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + 
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")", sep = ""))) + 
  theme_bw() +
  theme(strip.text.x = element_blank()) +
  facet_wrap(~Participant)

```
Figure \@ref(fig:RewardFacets) shows the individual data for each of the participants in this experiment. Each plot shows where the participants stood on each trial for each of the distances at which they were tested with the different conditions being colour coded. 


``` {r RewardFacets, include = T, echo = F, fig.height = 3, fig.cap = "Each facet in this plot shows the decision on a single trial about where to stand and how the participant had opted to split the reward. The x-axis corresponds to the participants chance of success had they stood at the centre with the y-axis showing the normalised standing potision."}
df_Rewards <- model_data %>%
  group_by(Participant) %>%
  mutate(temp = as.numeric(as.factor(Norm_Delta)),
         slab_measures = factor(temp, labels = c("90%", "75%", "25%", "10%"))) %>% 
  select(-temp) %>% 
  ungroup() %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(Norm_Delta))

df_Rewards %>% 
  ggplot(aes(overall_hoop_pos, Norm_Dist,
             colour = Gamble_Type)) + 
  geom_jitter(alpha = .3,
              width = .05,
              height = .05) +
  facet_wrap(~Participant, ncol = 4) + 
  scale_x_continuous(expression(paste("Hoop Delta (", Delta, ")")),
                     breaks = unique(df_Rewards$overall_hoop_pos),
                     labels = unique(df_Rewards$slab_measures)) +
  scale_y_continuous("",
                     breaks = c(0,1),
                     labels = c("Centre", "Side")) +
  see::scale_color_flat() +
  theme_bw() + 
  theme(
    strip.text.x = element_blank(),
    legend.position = "bottom",
    axis.text.x = element_text(size = 8),
    panel.grid.minor.x = element_blank()
  )

model_data <- model_data %>% 
  mutate(close_equal = ifelse(dist_type == "close" & Gamble_Type == "Equal", 1, 0),
         close_unequal = ifelse(dist_type == "close" & Gamble_Type == "Unequal", 1, 0),
         far_equal = ifelse(dist_type == "far" & Gamble_Type == "Equal", 1, 0),
         far_unequal = ifelse(dist_type == "far" & Gamble_Type == "Unequal", 1, 0))

```

A Bayesian Beta Regression was carried out in order to investigate whether opting for an unequal reward structure in this task would facilitate the use of a more optimal strategy in the _Throwing Task_. The predictors in this model were whether the participant had opted for an _Equal_ or _Unequal_ (Gamble_Type), whether the hoops were _Close_ or _Far_ (dist_type), and the interaction between these. Additionally, random effects of participant were included for all predictors. The predicted value was the normalised standing position, with 0 representing the centre and 1 being next to one of the hoops.  

### Modelling


The priors in for this model were set so as to reduce the likelihood of extreme values (i.e., close to 0% or 100%). Until this experiment, we had not run a version of this task in which the different targets could have different value associated with them. 

As the prior predictive checks demonstrated that the model was able to retrieve the prior, and the priors were suitably flat, the model was conditioned on the raw data from the experiment. The model looked as follows with the prior predictive checks having the additional "sample_prior" argument set to "_only_": 


```{r RewardsParameters, echo = F, include = T}
R_prior <- c(set_prior("student_t(3,0,1.25)",
                       class = "b",
                       coef = "close_equal"),
             set_prior("student_t(3,0,1.25)",
                       class = "b",
                       coef = "far_equal"),
             set_prior("student_t(3,0,1.25)",
                       class = "b",
                       coef = "close_unequal"),
             set_prior("student_t(3,0,1.25)",
                       class = "b",
                       coef = "far_unequal"))

R_iter = 10000
R_control = list(adapt_delta = .99, max_treedepth = 15)


```


```{r GamblingPriorsOnly, include = F, echo = F, message = F, warning = F, results = "hide", cache = TRUE}
Reward_priors <- brm(
  Norm_Dist ~ 0 + close_equal + close_unequal + far_equal + far_unequal +
    (0 + close_equal + close_unequal + far_equal + far_unequal|Participant),
  data = model_data,
  family = "beta",
  prior = R_prior,
  chains = 1,
  iter = R_iter,
  warmup = R_iter/2,
  sample_prior = "only",
  control = R_control)
```


``` {r RewardPriorsPlt, include = F, echo = F}
plt_RewardPriors <- draws_factor(Reward_priors) %>% 
  mutate(prop = boot::inv.logit(estimate)) %>% 
  ggplot(aes(prop,
             colour = Gamble_Type,
             fill = Gamble_Type)) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  geom_density(alpha = .3,
               kernel = "biweight",
               bw = .1) +
  scale_x_continuous(expression(Phi),
                     limits = c(-.1, 1.1),
                     breaks = seq(0,1,.25)) +
  guides(colour = guide_legend(title = "Gamble Type"),
         fill = guide_legend(title = "Gamble Type")) +
  facet_wrap(~Dist_Type) + 
  theme_bw()

save(Reward_priors, file = "Rewardpriors")
```

```{r RewardModel, include = T, echo = T, message = F, warning = F, results = "hide", cache = TRUE}
# run model 
Reward_m1 <- brm(
  Norm_Dist ~ 0 + close_equal + close_unequal + far_equal + far_unequal +
    (0 + close_equal + close_unequal + far_equal + far_unequal|Participant),
  data = model_data,
  family = "beta",
  prior = R_prior,
  chains = 1,
  iter = R_iter,
  warmup = R_iter/2,
  control = R_control)

```

```{r, echo = F, include = T}
save(Reward_m1, file = "Rewardmodel")
summary(Reward_m1)$fixed

```

```{r setup_PltRewardestimates, echo = F, include = F, warning = F}
# get draws 
draws_df <- draws_factor(Reward_m1) %>% 
  mutate(prop = boot::inv.logit(estimate))

# make separate plots for main effects and interactions 
plt_dist <- draws_df %>%
  ggplot(aes(prop,
             colour = Dist_Type,
             fill = Dist_Type)) +
  geom_density(alpha = .3) +
  # see::scale_color_material() +
  # see::scale_fill_material() +
  scale_colour_viridis_d() + 
  scale_fill_viridis_d() +
  theme_bw() +
  guides(colour = guide_legend(title = "Distance Type"),
         fill = guide_legend(title = "Distance Type")) +
  scale_x_continuous("")

plt_gamble <- draws_df %>% 
  ggplot(aes(prop, 
             colour = Gamble_Type,
             fill = Gamble_Type)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  theme_bw() +
  guides(colour = guide_legend(title = "Gamble Type"),
         fill = guide_legend(title = "Gamble Type")) +
  scale_x_continuous("")

plt_inter <- draws_df %>%
  ggplot(aes(prop, 
             colour = Gamble_Type,
             fill = Gamble_Type)) + 
  geom_density(alpha = .3) + 
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  theme_bw() +
  guides(colour = guide_legend(title = "Gamble Type"),
         fill = guide_legend(title = "Gamble Type")) +
  scale_x_continuous(expression(Phi),
                     limits = c(-.1, 1.1),
                     breaks = seq(0,1,.25)) + 
  facet_wrap(~Dist_Type)

# get common guide 
Reward_legend <- g_legend(plt_inter)

bottom <- grid.arrange(arrangeGrob(plt_gamble + theme(legend.position = "none"), plt_inter + theme(legend.position = "none")),
                       Reward_legend, widths=c(4.5, 1))

```

```{r PltRewardestimates, echo = F, include = T, warning = F, fig.height = 3, fig.cap="This plot shows the posterior distribution of standing positions for the Reward study"}
grid.arrange(arrangeGrob(plt_RewardPriors + theme(legend.position = "none"), plt_inter + theme(legend.position = "none")), Reward_legend, ncol = 2, widths = c(6,1))
#plt_inter
```

```{r RewardHDI, include = F, echo = F}
Rewards_hdi_Dist <- draws_df %>% 
  group_by(Dist_Type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2])

Rewards_hdi_Gamble <- draws_df %>% 
  group_by(Gamble_Type) %>% 
  summarise(lower = hdci(prop)[,1],
            mu = mean(prop),
            upper = hdci(prop)[,2])

Rewards_hdi_interaction <- draws_df %>% 
  group_by(Gamble_Type, Dist_Type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2])

```

```{r Probxgreaterthany, echo = F, include = F}
Pabove0_Dist <- draws_df %>% 
  group_by(.iteration, Dist_Type) %>% 
  summarise(mu = mean(prop)) %>%
  spread(Dist_Type, mu) %>% 
  mutate(diff = Far - Close,
         above0 = ifelse(diff > 0, 1, 0))

Pabove0_int <- draws_df %>% 
  group_by(.iteration, Dist_Type, Gamble_Type) %>% 
  summarise(mu = mean(prop)) %>% 
  mutate(inter = paste(Dist_Type, Gamble_Type, sep = "_")) %>% 
  ungroup() %>% 
  select(mu, inter, .iteration) %>%
  spread(inter, mu) %>% 
  mutate(diff_Far = Far_Unequal - Far_Equal,
         diff_Close = Close_Unequal - Close_Equal,
         above0Far = ifelse(diff_Far > 0, 1, 0),
         above0Close = ifelse(diff_Close > 0, 1, 0))

```
As can be seen in Figure \@ref(fig:PltRewardestimates), participants in this task were more likely to stand towards one of the side hoops when the hoops were far apart (mean of `r round(Rewards_hdi_Dist[2,3], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_Dist[2,2], digits = 3)` , `r round(Rewards_hdi_Dist[2,4], digits = 3)`|) than when they were close together (mean of `r round(Rewards_hdi_Dist[1,3], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_Dist[1,2], digits = 3)` , `r round(Rewards_hdi_Dist[1,4], digits = 3)`|) with P(Far > Close|data) = `r paste(round(mean(Pabove0_Dist$above0), digits = 3)*100, "%", sep = "")`. This suggests that participants were sensitive to the addition of a reward in this experiment.

In addition to this, the interaction of Distance Type with Gamble Type was in the direction of an asymmetrical reward structure pushing participants to make more optimal decisions with participants standing closer to one of the targets when they opted for an unequal split (mean of `r round(Rewards_hdi_interaction[4,4], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_interaction[4,3], digits = 3)` , `r round(Rewards_hdi_interaction[4,5], digits = 3)`|) than when they had opted for an equal split (mean of `r round(Rewards_hdi_interaction[2,4], digits = 3)`, 95% HPDI of |`r round(Rewards_hdi_interaction[2,3], digits = 3)` , `r round(Rewards_hdi_interaction[2,5], digits = 3)`|) with P(Unequal > Equal|data) = `r paste(round(mean(Pabove0_int$above0Far), digits = 3)*100, "%", sep = "")`. 


### Raw Accuracy 

```{r RewardRawAcc, include=T, echo=F, fig.width = 10, fig.height = 3, fig.cap = "This figure shows the average earnings participants achieved at each distance. Each line represents a participant The facets represent the two different conditions."}
# Load data with accuracy
load("../Analyses/Experiment_3_Unequal_Reward/scratch/data/df_part2")

# sort out data 
df_part2 <- df_part2 %>%
  ungroup() %>% 
  group_by(Participant) %>% 
  mutate(slab_measures = factor(HoopDelta, labels = c("C", "75%", "25%", "F"))) %>% 
  ungroup() %>% 
  group_by(slab_measures) %>% 
  mutate(overall_hoop_pos = mean(HoopDelta))

# make plot
df_meanhoop <- df_part2 %>% 
  group_by(slab_measures) %>% 
  summarise(mean_pos = mean(HoopDelta))

df_part2 %>% 
  group_by(Participant, slab_measures, overall_hoop_pos) %>% 
  summarise(mu = mean(Winnings)) %>% 
  ggplot(aes(slab_measures, mu)) + 
  geom_path(aes(group = Participant),
            alpha = .3) +
  theme_bw() + 
  scale_y_continuous("Average Earnings per trial",
                     breaks = seq(0,40,5),
                     labels = c("0p", "5p", "10p", "15p", "20p", "25p", "30p", "35p", "40p")) +
  scale_x_discrete(expression(paste("Hoop Delta (", Delta, ")", sep = ""))) +
  theme(axis.text = element_text(size = 8),
        axis.title.y = element_text(size = 8),
        axis.title.x = element_text(size = 8),
        panel.grid.minor = element_blank())

```

# Probability Matching Experiment

## Note
This experiment was cut from the main body of the paper as it is the only experiment to make use of the _Detection Task_ from @clarke2015failure. The logic behind this experiment was the same as in the experiments discussed in the main paper in that we investigated a potential asymmetry in the design that may alter the way in which participants approached this task. 

## Introduction 

In this experiment, we made the probability of the target appearing in one of the two locations higher (80%) than the other (20%). When predicting which of two events is about to occur (for instance, whether a blue or yellow light would illuminate on a particular trial), people tend to match the underlying probability of each event, a tendency called Probability Matching [@Koehler2010; @Goodnow1955; @Vulkan2000ProbMatching]. If event A occurs 80% of the time, and option B only 20% of the time, the best course of action is to predict event A every time, as this would result in an average accuracy rate of 80%. People, in general, tend to instead select each option in proportion to its likelihood of success, yielding an average success rate (in this example) of only 68%. 

Probability matching may be due in part to a misunderstanding of probability [@REIMERS201811], but it has also been argued to reflect a reasonable tendency to seek out and exploit patterns in sequential events [@gaissmaier2008smart; @wolford2004searching]. @Gao2015 argue that when a person selects the more likely option on every trial (a strategy known as _Maximising_), they are accepting a loss for a minority of trials (20% in this example). As demonstrated in @KahnemanChoicesValuseFrames, people tend to find certain loss very aversive, so a strategy that includes a certain loss may be disregarded as a “solution” to the problem at hand [@Goodnow1955]. The only way to avoid this loss is to make an attempt to detect and exploit any potential pattern. In the context of deciding where to fixate between two potential targets locations, in both @morvan2012human and @clarke2015failure, the probability for each location to contain the target target was 50%. Given the philosophical and experimental arguments described above, participants may react to this choice between equally likely options with idiosyncratic pattern-seeking behavior [@gaissmaier2008smart; @yellott1969probability; @wolford2004searching]. In @clarke2015failure, this may have interfered with participants using information about their own ability to make decisions. Instead, they may have been attempting to figure out patterns in the sequence of targets to make better guesses about which one was likely to be the target on each trial. Adding in a bias for one option to be more likely may cater to people’s tendency to seek out, and attempt to exploit, patterns. It also breaks the Buridan’s Ass deadlock and makes the decision about which of two goals to attempt easier. In this Experiment, unequal probability was introduced into the paradigm used by @clarke2015failure in order to investigate 1) whether participants would make use of probability information in deciding where to fixate, and 2) whether probability manipulations might facilitate the decision about whether to fixate between two potential target locations or to look directly at one of them. If so, it would suggest that at least part of the reason for the poor decisions observed previously was the (unnatural) balance between the two potential targets. Additionally,this experiment will provide a useful replication of both the fixations decision results [@morvan2012human; @clarke2015failure] and the probability matching tendency [@Vulkan2000ProbMatching] in a new context.


## Methods 

### Power Analysis
```{r load data, include = F, echo = F, message = F, warning = F}
load("../power/Prob_match/scratch/df_resample")
```

For this experiment, data from ($N$ = 12) @clarke2015failure, which acted as a substitute for the Symmetric condition, and an unpublished pilot study ($N$ = 11), which followed the same rules as the Bias condition in the main experiment, were resampled in order to investigate how often participants would fixate one of the side boxes given it was more likely to contain the target for the Bias condition. For the Symmetric condition, we followed the same rule as the in the paper and classified the most likely box as the side box that participants fixated most often (Figure \@ref(fig:ProbOGData). The main interest for our purposes was the proportion of time participants fixated the most likely box.

```{r ProbOGData, include=T, echo=F, fig.height = 3, fig.cap="Boxplots to show the proportion of trials in which the participants fixated the each box by Condition and Distance type"}
plt_data <- df_resample %>%  
  ungroup() %>% 
  group_by(Participant, Condition, Dist_type) %>% 
  mutate(centre = ifelse(box_type == "Centre", 1, 0),
         ML = ifelse(box_type == "Most", 1, 0),
         LL = ifelse(box_type == "Least", 1, 0),
         n = n()) %>%
  summarise(centre = mean(centre),
            ML = mean(ML),
            LL = mean(LL)) %>% 
  gather(centre:LL,
         key = "prop_type",
         value = "proportion") %>%
  mutate(prop_type = factor(prop_type, c("centre", "ML", "LL"),
                            labels = c("Centre", "Most Likely", "Least Likely"))) %>% 
  ggplot(aes(Dist_type, proportion,
             fill = Condition,
             colour = Condition)) + 
  geom_boxplot(alpha = .3) + 
  geom_point(alpha = .3, position = position_jitterdodge(.1)) +
  facet_wrap(~prop_type) +
  see::scale_color_flat() +
  see::scale_fill_flat() +
  theme_bw() +
  scale_x_discrete("Distance Type") +
  theme(axis.title.y = element_blank())
plt_data
```

The data were coded so that a fixation was classified as either being to the Most likely box or not. We then resampled (with replacement) these data by selecting a random $N$ participants from each condition (ranging from 2 to 20) then sampling 300 trials from these participants from the different data sets. This was done 5000 times to estimate the expected difference between the Symmetric and Bias conditions in terms of proportion of fixations to the Most likely side and the associated certainty around these values.

As can be seen in the figure below, the uncertainty surrounding the estimate for the difference between the groups appears to plateau somewhere around 15 participants. The shaded region represents a 95% Highest Density Interval (HDI) for the distribution of differences simulated through resampling. As such, the sample size of 18 in the main experiment appears to be ample in order to detect this difference and increasing the sample size above this value does not add more to the certainty, as can be seen in Figure \@ref(fig:ProbPowerPlots). 

``` {r prob matching power, include = F, echo = F, cache = T}
load("../power/Prob_match/scratch/df_resample")

# sampling
n_trials <- 300 # seq(100, 450, 50)
n_subs <- seq(2, 20, 1)
n_iter <- 5000
conditions <- unique(df_resample$Condition)

# setup data.table 
n <- length(n_subs) * length(conditions) * n_iter
df_sample <- data.table::data.table(iter = rep(0, n),
                                    n_sub = rep(0, n),
                                    n_trials = rep(0, n),
                                    Condition = rep("", n),
                                    Most = rep(0, n))

# run loop
counter <- 0
for(ii in 1:n_iter){
  # loop conditions
  for(cond in conditions){
    ss_cond <- df_resample %>%
      filter(Condition == cond)
    
    # loop n trials
    for(trials in n_trials){
      # loop n subjects
      for(sub in n_subs){
        # get random participants
        subj <- sample(ss_cond$Participant, sub, replace = T)
        
        # get data
        ss_sub <- ss_cond %>%
          filter(Participant %in% subj)
        
        # set this for a stopping point
        # final_sub <- tail(unique(ss_sub$Participant), n = 1)
        
        # initialise vector
        samp <- c()
        
        # loop through participants
        count <- 0
        for(unique_sub in subj){
          count <- count + 1
          # who are we sampling from?
          sampling <- ss_sub %>%
            filter(Participant == unique_sub)
          
          # sample random trials
          samp <- c(samp, sample(sampling$Most, trials, replace = T))
          
          # check if we're adding this to the data frame
          if(count == length(subj)){
            counter <- counter + 1
            df_sample[counter, iter := ii]
            df_sample[counter, n_sub := sub]
            df_sample[counter, n_trials := trials]
            df_sample[counter, Condition := cond]
            df_sample[counter, Most := mean(samp)]
          }
        }
      }
    }
  }
}



```

```{r ProbPowerPlots, include = T, echo = F, fig.height = 3, fig.cap="Plot to show how the 95\\% HDI of the estimated difference in fixations to the most likely box changed with an increase in participants"}
# maybe just the hdi stuff?
# do I want to rerun the sim with dist type added in?
plt_resampling <- df_sample %>% 
  spread(Condition, 
         Most) %>% 
  mutate(diff = Bias - Symmetric) %>% 
  group_by(n_sub) %>% 
  summarise(mu = mean(diff),
            lower = hdi(diff)[1],
            upper = hdi(diff)[2]) %>% 
  ggplot(aes(n_sub, mu)) + 
  geom_line() + 
  geom_ribbon(aes(ymin = lower,
                  ymax = upper),
              alpha = .3) +
  theme_bw() + 
  scale_x_continuous("Sample Size") + 
  scale_y_continuous("Difference in fixations to more \n likely side (Bias - Symmetric)")

plt_resampling
```


### Participants
16 Participants (two Male) were recruited from the University of Aberdeen community with an average age of 22.75 (between 19 and 29). Each participant was reimbursed £10 for their time. 

### Procedure
The experiment followed a similar procedure to the "_Detection Task_" from @clarke2015failure. In the first session, we measured visual acuity which lasted approximately 30 minutes. This was followed by a second session in which the participants performed the actual decision task which lasted approximately 40-50 minutes.

The experiment took place in a darkened room on a desktop computer. An Eyelink 1000 (version 4.594)(SR Research ltd, Mississauga, Ontario, Canada) was used to record eye position at 1000Hz. In each session, a 5 point calibration was carried out with additional calibration and validation sequences prior to each block, and if the participants had broken fixation ten times cumulatively or for five trials in a row. The stimuli were displayed on a CRT monitor (resolution 1920 $X$ 1080 pixels) using Matlab 7.9.0 (R2009b) with Psychtoolbox [@brainard1997psychophysics; @pelli1997videotoolbox] and EyelinkToolbox functions [@cornelissen2002eyelink]. A chin rest with forehead bar was used to ensure participants maintained a viewing distance of  $\approx$ 47cm.

In the first session, participants were instructed to remain fixated at the center and identify a letter that would appear in one of two boxes. At the start of each trial,participants fixated a central black cross on a grayscale background and pressed the spacebar. The cross was flanked by two boxes which were a lighter shade of grey and occupied 1◦ of visual angle. After a stable fixation had been maintained for 700ms, the target would appear in one of the two boxes for 500ms. The target was a white letter, 0.4&deg; of visual angle, drawn using the Sloan font as these letters are generally of equal recognisability at different viewing angles[@sloan1952comparison]. Ten letters were used and one was selected randomly on each trial to be the target letter. Participants were then presented with a screen prompting them to report which letter was presented by clicking on the corresponding character. An illustration of each trial in Session 1 can be seen in Figure \@ref(fig:ProbSession1).

```{r ProbPart1, include=T, echo=F, fig.height = 3, fig.cap="The sequence for every trial in Session 1"}

plt_session1Prob <- readPNG("../Figures/Experiment_sup_Prob/Part2_Trial.png")
grid.raster(plt_session1Prob)

```

The boxes were presented on either side of the fixation cross at several different eccentricities (3.1&deg;,4.3&deg;,5.8&deg;,7.5&deg;,9.3&deg;,11.1&deg;,12.5&deg;,& 13.7&deg;). Each eccentricity was repeated 12 times in a row before moving on to another eccentricity (the order of these sets of 12 eccentricities was random),for a total of 96 trials in a block.

After each block, participants were offered a break before recalibrating and moving onto the next block. Participants completed four of these blocks. The data from this first session were used to tailor the separations that would be used for the second session. A “switch-point” was calculated for each participant based on their Session 1 performance. This was then used as an anchor point from which to calculate the other separations to be used in Session 2. The switch-point for each participant was the separation at which the participant was 68.5% accurate. The accuracy level of 68.5% is the mid-point between 55% and 82%; these two values are the points at which participants should switch from fixating the central box to the side box in the Symmetric and Bias conditions, respectively(see below for more details).


```{r ProbSession2, include=T, echo=F, fig.height = 3, fig.cap="The sequence for every trial in Session 2"}

plt_fig2 <- readPNG("../Figures/Experiment_sup_Prob/Part2_Trial.png")
grid.raster(plt_fig2)

```

From this point, 6 other separations were calculated $\pm$ 1&deg;, 2&deg;, and 3&deg;. There were two other points included which acted as anchors, one being a very large distance (19.4&deg;), the other very small (1.9&deg;). In Session 2, participants started by fixating a cross that appeared above where the boxes would appear. The cross would appear at the midpoint between the centre box and either the left or right box with an equal probability. Once they had fixated the cross, they were instructed to press the spacebar. After a stable fixation was detected for 700ms, three boxes would be presented (Figure \@ref(fig:ProbSession2)). One box would appear in the centre with the other two spaced equally on either side with separations calculated as above, based on Session 1 performance for that particular participant. Once these boxes had appeared, participants were instructed to fixate one of the three boxes. Participants were told that the target would never appear in the central box, however they could choose to fixate this location. After they had fixated one of the boxes, the letter appeared in one of the side boxes for 500ms. After this, the 10 letter stimuli were presented on screen for them to select which letter they had seen. Each separation appeared 10 times within a block, for a total of 90 trials. The order of separations was randomised (rather than blocked, as it was in Session 1).

Prior to each block, participants were told how likely each box was to contain the target. There were two levels of probability; one in which each box was equally likely to contain the target on each trial (Symmetric), and one in which one side would contain the target 80% of the time (Biased). Each participant took part in both the Symmetric and Biased conditions. Participants completed one condition for 4 blocks before moving on to the other condition. This order was counterbalanced across participants. Additionally, the side that was more likely to contain the target was also counterbalanced, to control for any bias to look to one side over the other.

### Optimal Strategy

The optimal strategy is to fixate whichever of the three boxes maximises expected accuracy. We refer to a participant’s expected accuracy if they had followed the optimal strategy, as “optimal accuracy”.  Using the psychometric curves fit to each participant’s Session 1 data, we can calculate how likely a given person is to detect the target at various distances. This was then used to predict how accurate that same person would be if they had fixated one of the side boxes, or the centre box, by using their average accuracy for a given distance from the left and right box ($Bl$ and $Br$ respectively) in Session 1. The fixation choice that gave the greatest expected accuracy was then selected as the optimal decision for the respective box separation. As probability was also a factor in this experiment, this had to be accounted for by multiplying the chance of detecting the target in either box by the probability that the box would contain the target ($Pl$ and $Pr$). The formula for expected accuracy is therefore: $(Bl \times Pl) + (Br \times Pr)$. Expected accuracy given optimal fixations differs between the Symmetric and Biased conditions. For example, in the Symmetric condition, participants could expect a 55% success rate if they fixated the optimal location when the targets were far apart. This value comes from assuming they would be $\approx$ 100% accurate for the fixated box, and at chance level for the non-fixated box (10%). This gives us $(1 \times 0.5) + (0.1 \times 0.5) = 55%$. To get the lower limit for the Bias condition, we simply change the chance for each box to contain the target (additionally, we assume that the participant would fixate the most likely box). This gives us $(1 \times 0.8) + (0.1 \times 0.2) = 82%$. This same formula can be used to calculate expected accuracy had the participant fixated the central box. In this case, $Bl = Br$, and so would simply be put into the formula using the appropriate values for $P$, as demonstrated above. 

### Analysis
All analysis for this experiment follow the same procedures as described in previous sections. 

## Results
```{r ProbProp, include = T, echo = F, fig.height = 3, fig.cap="These boxplots show the proportion of the time that participants fixated the centre (left panel), most-likely (central panel) and least-likely (right panel) box. In the Bias condition, most likely was the box with and 80\\% probability of containing the target. In the Symmetric condition, most-likely was whichever side box a given participant had fixated the most. The Close and Far distinction is baseed on when participants should swith from fixating the central box to the side box. Note that for some participants, expected performance differences between the Centre and Side strategies for the closest separation was negligible. However, all participants should have fixated the Side boxes in the Far condition, where the performance advantage of doing so was substantial."}

plt_prop_fix <- readPNG("../Figures/Experiment_sup_Prob/boxes_prop_CML.png")
grid.raster(plt_prop_fix)

```

The choices made by participants on where to look are summarized in Figure \@ref(fig:ProbProp). The optimal strategy in this situation is to fixate the central box when the two sides boxes are near (i.e., when their distance from centre, $\Delta$, is closer than each participant’s switch point). When they are far (i.e. delta is greater than the switch point), a participant behaving optimally should fixate either of the side boxes when in the symmetric condition, and the most-likely box when in the biased condition. The first panel of Figure \@ref(fig:ProbProp) clearly shows that our participants did not follow this strategy.  However, while adding a bias to the location of the target did not help participants to behave optimally, we can see that it did have an effect on their behavior, as they were much less likely to fixate the central box. Furthermore, when fixating one of the two side squares, they fixated the square that was most likely to contain the target (Figure \@ref(fig:ProbProp), central and right panel) almost all the time

```{r ProbExpAcc, include = T, echo = F, fig.height = 3, fig.cap = "These show the expected accuracy for each participant (black lines) plotted against $\\Delta$. The different shaded regions show the minimum (orange) and maximum (green) expected accuracy rates for all participants. $\\Delta$ is the separations at which each participant was tested rescaled to be between 0 and 1"}
plt_prop_expAcc <- readPNG("../Figures/Experiment_sup_Prob/region_performance.png")
grid.raster(plt_prop_expAcc)
```

We can also look at how the choice of where to look influenced participants’ accuracy. We use the target detection models from Session 1 to calculate the expected accuracy given either (i) an optimal, (ii) counter-optimal, or (iii) the observed strategy. This measure eliminates the variance present in the actual accuracy data due to the random location of the target and participant’s guessing the correct answer by chance. These data are summarised in Figure \@ref(fig:ProbExpAcc). We can see that in the symmetric condition, with the exception of one individual, our participants behaved in a way that gave rise to expected accuracies much closer to the counter-optimal strategy than optimal. Expected accuracy was higher in the biased condition, with four participants reaching optimal performance, with the rest ending up somewhere between the optimal and counter-optimal. 

### Session 1 Accuracy 
```{r probSession1, include = T, echo = F, message = F, warning = F, fig.height = 3, fig.cap = "This figure shows the rate at which participants correctly identified the target letter presented within a box placed some distance from the centre with the lines showing a logistic regression on these data."}
# load in part 1 data 
load("../Analyses/Experiment_sup_Prob/scratch/new_data/Part_1_data_nar")

# function 
get_VisDegs <- function(separation,distance){
  ((2*atan2(separation,(2*distance)))*180)/pi
}

# Any constants 
Screen_dist <- 53
ppcm <- 1920/54.4

df %>% 
  mutate(degs = round(get_VisDegs(separation/ppcm, Screen_dist), digits = 1)) %>%
  group_by(participant, separation) %>% 
  mutate(acc = mean(accuracy)) %>% 
  ggplot(aes(degs, accuracy)) + 
  geom_smooth(method = "glm",
              method.args = list(family = binomial(mafc.logit(10))),
              se = F,
              colour = "black") +
  geom_point(aes(x = degs, y = acc)) + 
  theme_bw() +
  theme(strip.text.x = element_blank()) + 
  scale_y_continuous("Accuracy", labels = scales::percent_format(accuracy = 1)) +
  scale_x_continuous(expression(paste("Visual Degrees (", degree, ")", sep = ""))) + 
  facet_wrap(~participant)
```

### Modelling
#### Prior Predictive checks
```{r Prob load data, include = F, echo = F}
# load data
load("../Analyses/Experiment_sup_Prob/modelling/BRMS/model_data/df_model")

# load function
source("extract_draws_functions/Prob_draws.R")

n_iter <- 2000
n_warmup <- n_iter/2
```

``` {r Setting Priors, include = F, echo = T}
# setting priors
Rewards_priors <- c(set_prior("student_t(3, 0, .5)", class = "b"),
                    set_prior("student_t(3, 0, .5)", class = "b", coef = "bias_typeSymmetric"),
                    set_prior("student_t(3, 0, .5)", class = "b", coef = "bias_typeSymmetric:dist_typeFar"),
                    set_prior("student_t(3, 0, .5)", class = "b", coef = "dist_typeFar"),
                    set_prior("student_t(3,0,1)", class = "Intercept"))
```

```{r ProbPriors, include = F, echo = F, message = F, warning = F, results = "hide", cache = TRUE}
ProbPriors_model <- brm(Ml_fix ~ (bias_type + dist_type)^2 + (dist_type * bias_type|participant),
                        data = df_model,
                        family = "bernoulli",
                        prior = Rewards_priors,
                        chains = 1,
                        iter =   n_iter,
                        warmup = n_warmup,
                        sample_prior = "only")
```


```{r setupProbPriorsPlt, include = F, echo = F}
ProbPriordraws <- Prob_draws(ProbPriors_model)

plt_ProbPrior_interaction <- ProbPriordraws$plt_estimates + 
  scale_x_continuous("Probability of fixating the Most likely side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  theme_bw() + 
  guides(fill = guide_legend("Bias Type"),
         colour = guide_legend("Bias Type"))

prior_draws <- ProbPriordraws$draws_df %>%
  mutate(prop = boot::inv.logit(estimate)) %>%
  select(-estimate) 

plt_ProbPrior_meDist <- prior_draws %>%
  ggplot(aes(prop,
             colour = Dist_type,
             fill = Dist_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() +  
  theme(axis.title.x = element_blank()) +
  labs(fill = "Distance Type",
       colour = "Distance Type") +
  #       x = "Proportion of Fixations to Most Likely Side") + 
  scale_colour_viridis_d() + 
  scale_fill_viridis_d() +
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

plt_ProbPrior_meBias <- prior_draws %>%
  ggplot(aes(prop,
             colour = Bias_type,
             fill = Bias_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() + 
  theme(axis.title.x = element_blank()) +
  labs(fill = "Bias Type",
       colour = "Bias Type") +
  #       "Proportion of Fixations to Most Likely Side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

# get common guide 
ProbPrior_legend <- g_legend(plt_ProbPrior_interaction)

bottom <- grid.arrange(arrangeGrob(plt_ProbPrior_meBias + theme(legend.position = "none"), plt_ProbPrior_interaction + theme(legend.position = "none")), 
                       ProbPrior_legend, widths=c(4.5, 1))

```

```{r ProbPriorsPlt, echo = F, fig.height = 5, fig.cap="Prior Predictive checks for the Probability matching experiment"}
gridExtra::grid.arrange(plt_ProbPrior_meDist, bottom, heights = c(2, 4))
```

```{r ProbModel, include = T, echo = T, message = F, warning = F, results = "hide", cache = TRUE}
ProbModel <- brm(Ml_fix ~ (bias_type + dist_type)^2 + (dist_type * bias_type|participant),
                 data = df_model,
                 family = "bernoulli",
                 prior = Rewards_priors,
                 chains = 1,
                 iter = n_iter,
                 warmup = n_warmup)
```

```{r, echo = F, include = T}

summary(ProbModel)$fixed

```

```{r getdraws, include = F, echo = F}
Probdraws <- Prob_draws(ProbModel)

```


```{r Get Values, include = F, echo = F}
# get draws
draws <- Probdraws$draws_df %>%
  mutate(prop = boot::inv.logit(estimate)) %>%
  select(-estimate)

# HDI_values 
HDI_Bias <- draws %>%
  group_by(Bias_type) %>%
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2],
            med = median(prop))
HDI_Dist <- draws %>% 
  group_by(Dist_type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2],
            med = median(prop))
HDI_interaction <- draws %>% 
  group_by(Bias_type, Dist_type) %>% 
  summarise(lower = hdi(prop)[,1],
            mu = mean(prop),
            upper = hdi(prop)[,2],
            med = median(prop))

# Diff values 
Diff_Bias <- draws %>% 
  spread(Bias_type, 
         prop) %>%
  mutate(diff = Bias - Symmetric,
         Bias_bigger = ifelse(diff > 0, 1, 0))
D_bias_lower <- hdci(Diff_Bias$diff)[,1]
D_bias_mu <- mean(Diff_Bias$diff)
D_bias_upper <- hdci(Diff_Bias$diff)[,2]
p_bias_bigger <- mean(Diff_Bias$Bias_bigger)

Diff_Dist <- draws %>% 
  spread(Dist_type, 
         prop) %>%
  mutate(diff = Far - Close,
         Far_bigger = ifelse(diff > 0, 1, 0))
D_dist_lower <- hdci(Diff_Dist$diff)[,1]
D_dist_mu <- mean(Diff_Dist$diff)
D_dist_upper <- hdci(Diff_Dist$diff)[,2]
p_far_bigger <- mean(Diff_Dist$Far_bigger)

Diff_Interaction_compdist <- Diff_Dist %>% 
  group_by(Bias_type) %>% 
  summarise(p_Far_bigger = mean(Far_bigger),
            lower = hdci(diff)[,1],
            mu = mean(diff),
            upper = hdci(diff)[,2],
            med = median(diff))
Diff_Interaction_compbias <- Diff_Bias %>% 
  group_by(Dist_type) %>% 
  summarise(p_Bias_bigger = mean(Bias_bigger),
            lower = hdci(diff)[,1],
            mu = mean(diff),
            upper = hdci(diff)[,2],
            med = median(diff))

```

The results of this model suggest that our participants were sensitive to the probability information (see figure \@ref{fig:ProbModelOutputs}). In the Biased condition, the average participant fixated the most likely target `r paste(round(HDI_Bias[1,3], digits = 3)*100, "%", sep = "")` of the time (95% HDPI of |`r paste(round(HDI_Bias[1,2], digits = 3)*100, "%", sep = "")`, `r paste(round(HDI_Bias[1,4], digits = 3)*100, "%", sep = "")`|), compared to `r paste(round(HDI_Bias[2,3], digits = 3)*100, "%", sep = "")` (95% HPDI of |`r paste(round(HDI_Bias[2,2], digits = 3)*100, "%", sep = "")`,`r paste(round(HDI_Bias[2,4], digits = 3)*100, "%", sep = "")`|) in the symmetric condition. The width of these intervals reflects a high degree of uncertainty in fixed effects, due to the range of behaviours exhibited by participants. None-the-less, the HPDI on the difference between these two conditions, |`r round(D_bias_lower, digits = 3)*100`%, `r round(D_bias_upper, digits = 3)*100`%| is largely positive and we can be reasonably confident (P(difference > 0 | data ) =  `r round(p_bias_bigger, digits = 3)*100`%) that the most-likely target is fixated more frequently in the biased condition. The distance between the square targets did not appear to have any consistent effect in the Symmetric condition, however there was a small decrease in fixations towards the “most likely” box when the boxes were far apart in the bias condition (dropping from `r round(HDI_interaction[1,4], digits = 3)*100`%, 95% HPDI of |`r round(HDI_interaction[1,3], digits = 3)*100`%,`r round(HDI_interaction[1,5], digits = 3)*100`%| to `r round(HDI_interaction[2,4], digits = 3)*100`%, 95% HPDI of |`r round(HDI_interaction[2,3], digits = 3)`%,`r round(HDI_interaction[2,5], digits = 3)*100`%|). As such, the difference between the conditions were much more pronounced in the close condition (P(Bias > Symmetric|data) = `r round(Diff_Interaction_compbias[1,2], digits = 3)*100`%) than in the far condition (P(Bias > Symmetric|data) = `r round(Diff_Interaction_compbias[2,2], digits = 3)*100`%). The width of these intervals reflects the range of performance that was exhibited by participants. However, this does show that participants generally made us of this probability information in order to decide where to fixate. 

```{r setupProbModelOutputs, include = F, echo = F}
plt_ProbPost <- Probdraws$plt_estimates + 
  scale_x_continuous("Probability of fixating the Most likely side") + 
  see::scale_color_flat() + 
  see::scale_fill_flat() +
  theme_bw() + 
  guides(fill = guide_legend("Bias Type"),
         colour = guide_legend("Bias Type"))

# Proportion fixations to most likely side 
plt_me_Bias <- draws %>% 
  ggplot(aes(prop,
             colour = Bias_type,
             fill = Bias_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() + 
  theme(axis.title.x = element_blank()) +
  labs(fill = "Bias Type",
       colour = "Bias Type") +
  #       "Proportion of Fixations to Most Likely Side") +
  see::scale_color_flat() + 
  see::scale_fill_flat() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

plt_me_Dist <- draws %>% 
  ggplot(aes(prop,
             colour = Dist_type,
             fill = Dist_type)) + 
  geom_density(alpha = .3) + 
  theme_bw() +  
  theme(axis.title.x = element_blank()) +
  labs(fill = "Distance Type",
       colour = "Distance Type") +
  #       x = "Proportion of Fixations to Most Likely Side") +
  scale_colour_viridis_d() + 
  scale_fill_viridis_d() + 
  scale_x_continuous(labels = scales::percent_format(accuracy = 1))

ProbPost_legend <- g_legend(plt_ProbPost)

bottom <- grid.arrange(arrangeGrob(plt_me_Bias + theme(legend.position = "none"), plt_ProbPost + theme(legend.position = "none")), 
                       ProbPost_legend, widths=c(4.5, 1))
# gridExtra::grid.arrange(plt_me_Dist, plt_me_Bias, plt_ProbPost)
```

```{r ProbModelOutputs, include = T, echo = F, fig.height = 5, fig.cap = "Posterior predictions for the probability matching experiment"}
gridExtra::grid.arrange(plt_me_Dist, bottom,  ncol = 1, heights = c(2,4))
```

## Discussion 
The results of this study suggest that participants were in fact sensitive to the difference in probability of each side containing the target and responded by fixating this location more often when there was a bias compared to the Symmetric conditions. However, the presence of a bias did not lead participants to utilising a more optimal strategy. It would appear that the introduction of a bias for one target distracted participants from discovering the optimal strategy. Instead, participants appeared to focus primarily on the probability aspect of the task. 

\newpage

# Bibliography
